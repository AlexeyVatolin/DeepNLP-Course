# Лекция 2
*Введение в PyTorch и реализация моделей Word2Vec*

Ноутбук на Colab можно найти [здесь](https://colab.research.google.com/drive/15tg6jTt1F0oR5PzFlcZNBnpPiu4se1m3)  
**Обратите внимание, ноутбук обновился в части про Word2Vec.**

**Задание**  
*Дедлайн: 09:00 GMT+3 15.03.18*  
1. Реализовать Skip-Gram модель.  
2. Реализовать Negative Sampling для CBoW модели.  
3. (По желанию) Разобраться со статьёй [Two/Too Simple Adaptations of Word2Vec for Syntax Problems
] (http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf) и реализовать Structured Skip-gram Model модель.

Дополнительные материалы:  
[On word embeddings - Part 1, by Sebastian Ruder](http://ruder.io/word-embeddings-1/)  
[On word embeddings - Part 2: Approximating the Softmax, by Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  
[cs224n "Lecture 2 | Word Vector Representations: word2vec" (video)](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s).  
[cs224n "Lecture 5: Backpropagation and Project Advice" (video)](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s).   