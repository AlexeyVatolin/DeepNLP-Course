{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 4 - RNNs v2.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "P59NYU98GCb9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.cuda import FloatTensor, LongTensor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-6CNKM3b4hT1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Рекуррентные сети, часть 2"
      ]
    },
    {
      "metadata": {
        "id": "O_XkoGNQUeGm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "metadata": {
        "id": "QFEtWrS_4rUs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Мы уже посмотрели на примеры применения рекуррентных сетей.\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg =x250)\n",
        "\n",
        "[(from The Unreasonable Effectiveness of Recurrent Neural Networks)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "---\n",
        "\n",
        "Перейдем к ещё одному варианту - sequence labeling (последняя картинка).\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS-Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "metadata": {
        "id": "EPIkKdFlHB-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "metadata": {
        "id": "TiA2dGmgF1rW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.cross_validation import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
        "all_tags = ['ADV', 'NOUN', 'ADP', 'PRON', 'DET',  '.', 'PRT', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ']\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d93g_swyJA_V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "metadata": {
        "id": "QstS4NO0L97c",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1G1N6xbWS6BL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Напомню, на прошлом занятии мы строили LSTM сеть, которая обрабатывала последовательности символов, и предсказывала, к какому языку относится слово. \n",
        "\n",
        "LSTM выступал в роли feature extractor'а, работающего с произвольного размера последовательностью символов (ну, почти произвольного - мы ограничивались максимальной длиной слова). Батч для сети имел размерность `(max_word_len, batch_size)`.\n",
        "\n",
        "Теперь мы опять хотим использовать такую же идею для извлечения признаков из последовательности символов - потому что последовательность символов же должна быть полезной для предсказания части речи, правда?\n",
        "\n",
        "Сеть должна будет запомнить, например, что `-ly` - это часто про наречие, а `-tion` - про существительное.\n",
        "\n",
        "Иллюстрация процесса:  \n",
        "![Char BiLSTM](https://image.ibb.co/fYDk9c/Fig2.png =x400)\n",
        "\n",
        "Но, конечно, только признаки, задаваемые набором символов в слове, совершенно бесполезны в многих случаях. Например, из-за омонимии:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "В результате у нас есть сразу две последовательности: последовательность слов в предложении и последовательность символов в словах.  \n",
        "*we_need_to_go_deeper.jpg*  \n",
        "\n",
        "Будем использовать словный LSTM, который работает на результатах работы символьного LSTM.\n",
        "\n",
        "Батч будет иметь размерности `(max_sentence_len, batch_size, max_word_len)`.\n",
        "\n",
        "Всё вместе будет работать так:\n",
        "\n",
        "![LSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig1/AS:391505383575554@1470353565232/Illustration-of-our-neural-network-for-Language-Modeling.png =x450)  \n",
        "from [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)\n",
        "\n",
        "---\n",
        "\n",
        "Оценим, для начала `max_word_len`: "
      ]
    },
    {
      "metadata": {
        "id": "NlYnKe9-PDHl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter \n",
        "    \n",
        "def find_max_len(counter, threshold):\n",
        "  sum_count = sum(counter.values())\n",
        "  cum_count = 0\n",
        "  for i in range(max(counter)):\n",
        "    cum_count += counter[i]\n",
        "    if cum_count > sum_count * threshold:\n",
        "      return i\n",
        "  return max(counter)\n",
        "\n",
        "word_len_counter = Counter()\n",
        "for sent in train_data:\n",
        "  for word in sent:\n",
        "    word_len_counter[len(word[0])] += 1\n",
        "    \n",
        "threshold = 0.99\n",
        "MAX_WORD_LEN = find_max_len(word_len_counter, threshold)\n",
        "\n",
        "print('Max word len for {:.0%} of words is {}'.format(threshold, MAX_WORD_LEN))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xQ_it05ecO3L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Сделаем не слишком хорошую вещь. Выкинем слишком длинные предложения"
      ]
    },
    {
      "metadata": {
        "id": "xh-ZxU9JbU7i",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sent_len_counter = Counter()\n",
        "for sent in train_data:\n",
        "  sent_len_counter[len(sent)] += 1\n",
        "\n",
        "threshold = 0.87\n",
        "MAX_SENT_LEN = find_max_len(sent_len_counter, threshold)\n",
        "\n",
        "print('Max sent length of {:.0%} of sentences is {}'.format(threshold, MAX_SENT_LEN))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "brVRJwwhcmnz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_data_len = len(train_data)\n",
        "train_data = [sent for sent in train_data if len(sent) <= MAX_SENT_LEN]\n",
        "print('Removed', train_data_len - len(train_data), 'too long sentences from train')\n",
        "\n",
        "test_data_len = len(test_data)\n",
        "test_data = [sent for sent in test_data if len(sent) <= MAX_SENT_LEN]\n",
        "print('Removed', test_data_len - len(test_data), 'too long sentences from test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xTai8Ta0lgwL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EfN1olf6RZne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Остаться должно вполне достаточно (но считаться всё будет быстрее).\n",
        "\n",
        "Теперь нужно сконвертировать данные."
      ]
    },
    {
      "metadata": {
        "id": "-LWXHmXGcotd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "\n",
        "def get_range(first_symb, last_symb):\n",
        "    return set(chr(c) for c in range(ord(first_symb), ord(last_symb) + 1))\n",
        "\n",
        "chars = get_range('a', 'z') | get_range('A', 'Z') | get_range('0', '9') | set(punctuation)\n",
        "char_index = {c : i for i, c in enumerate(chars)}\n",
        "\n",
        "def get_char_index(char, char_index):\n",
        "    return char_index[char] if char in char_index else len(char_index)\n",
        "\n",
        "def convert_data(data, max_sent_len, max_word_len):\n",
        "    X = np.zeros(<choose the dimensions>)\n",
        "    y = np.zeros(<choose the dimensions>)\n",
        "    for i, sent in enumerate(data):\n",
        "        for j, (word, tag) in enumerate(sent):\n",
        "            word = word[-max_word_len:]\n",
        "            <add word into X, y>\n",
        "    return X, y\n",
        "  \n",
        "X_train, y_train = convert_data(train_data, MAX_SENT_LEN, MAX_WORD_LEN)\n",
        "X_test, y_test = convert_data(test_data, MAX_SENT_LEN, MAX_WORD_LEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1SMmXMx5Rr5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Напишем генератор батчей (какую размерность они должны иметь?):"
      ]
    },
    {
      "metadata": {
        "id": "c835LEVERXzl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def iterate_batches(dataset, batch_size):\n",
        "    \"\"\"\n",
        "    returns X_batch - Variable with size (max_sentence_len, batch_size, max_word_len)\n",
        "    y_batch - Variable with size (max_sentence_len, batch_size)\n",
        "    \"\"\"\n",
        "    X, y = dataset\n",
        "    n_samples = X.shape[1]\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_idx = indices[start:end]\n",
        "        \n",
        "        X_batch = ...\n",
        "        y_batch = ...\n",
        "        \n",
        "        yield Variable(X_batch), Variable(y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_xoqYHMSUQt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Напишем достаточно удобный аналог метода `fit` в keras:"
      ]
    },
    {
      "metadata": {
        "id": "R24L0BPiDqfB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    model.train(not optimizer is None)\n",
        "    \n",
        "    batchs_count = math.ceil(data[0].shape[1] / batch_size)\n",
        "    \n",
        "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "        logits = model(X_batch)\n",
        "        \n",
        "        <calculate loss>\n",
        "        epoch_loss += loss.data[0]\n",
        "        \n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        <calculate accuracy>\n",
        "        \n",
        "        correct_count += cur_correct_count\n",
        "        sum_count += cur_sum_count\n",
        "        \n",
        "        print('\\r[{} / {}]: Loss = {:.5f}, Accuracy = {:.2%}'.format(i, batchs_count, \n",
        "              loss.data[0], cur_correct_count / cur_sum_count), end='')\n",
        "\n",
        "    return epoch_loss / batchs_count, correct_count / sum_count\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
        "        batch_size=32, val_data=None, val_batch_size=None):\n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
        "        \n",
        "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.5f}, Train Accuracy = {:.2%}'\n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, train_data, batch_size, None)\n",
        "            epoch_time = time.time() - start_time\n",
        "            output_info += ', Val Loss = {:.5f}, Val Accuracy = {:.2%}'\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, train_acc, val_loss, val_acc))\n",
        "        else:\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, train_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRB8tAOAa_YW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_embedding_dim, char_hidden_dim, \n",
        "                 lstm_hidden_dim, lstm_layers_count, tagset_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        <init layers>\n",
        "\n",
        "    def forward(self, chars):\n",
        "        <calculate logits>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEaWjN0qjFfe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "CHAR_VOCAB_SIZE = len(char_index) + 1\n",
        "CHAR_EMB_DIM = 24\n",
        "CHAR_HIDDEN_DIM = 64\n",
        "LSTM_HIDDEN_DIM = 128\n",
        "LSTM_LAYER_COUNT = 1\n",
        "\n",
        "model = LSTMTagger(<params>)\n",
        "model = model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20, \n",
        "    batch_size=128, val_data=(X_test, y_test), val_batch_size=2048)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TnEIc9byI-ZZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Masking\n",
        "Что, собственно, мы оптимизируем? Мы считаем сумму потерь для `max_sentence_len x batch_size` слов. Но ведь многие из них - нули.\n",
        "\n",
        "В keras есть специальный слой, который помогает не считать потери для нулевых сэмплов - [`Masking`](https://keras.io/layers/core/#masking).\n",
        "\n",
        "Хочется примерно такого же добиться. Для этого достаточно просто находить маску - матрицу из нулей и единиц с нулями там, где стоят нулевые элементы `y_batch`.  \n",
        "Дальше эту маску нужно умножить на потери для каждого элемента - и усреднить. В итоге потери для паддингов занулятся и не будут участвовать в подсчете потерь.\n",
        "\n",
        "**Задание** Добавьте Masking."
      ]
    },
    {
      "metadata": {
        "id": "PXUTSFaEHbDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png =x450)  \n",
        "from [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "metadata": {
        "id": "WzAozOANpnT1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Словные эмбеддинги\n",
        "\n",
        "**Задание** Только символьных эмбеддингов может быть недостаточно. Добавьте ещё словные эмбеддинги. Слова стоит приводить к нижнему регистру - признаки, связанные с регистром должны ухватываться символьный LSTM.\n",
        "\n",
        "Эти эмбеддинги можно просто сконкатенировать, а можно использовать гейт (как в LSTM). Например, по эмбеддингу слова предсказывать $o = \\sigma(w)$ - насколько он хорош и сочетать в такой пропорции с символьным эмбеддингом: $o \\odot w + (1 - o) \\odot \\tilde w$, где $\\tilde w$ - эмбеддинг слова, полученный по символьному уровню."
      ]
    },
    {
      "metadata": {
        "id": "N0AdvKbnW8yV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Byte-Pair Encoding\n",
        "\n",
        "Мы можем представлять слова одним индексом - и использовать словные эмбеддинги как строки матрицы эмбеддингов.  \n",
        "Можем считать их набором символов и получать словный эмбеддинг с помощью некоторой функции над символьными эмбеддингами.\n",
        "\n",
        "Наконец, можем ещё использовать промежуточное представление - как набор подслов.\n",
        "\n",
        "Несколько лет назад использование подслов предложили для задачи машинного перевода: [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909). Там использовалось byte-pair encoding.\n",
        "\n",
        "По сути это процесс объединения самых частотных пар символов алфавита в новый суперсимвол. Пусть у нас есть словарь, состоящий из такого набора слов:  \n",
        "`‘low·’, ‘lowest·’, ‘newer·’, ‘wider·’`   \n",
        "(`·` означает конец слова)\n",
        "\n",
        "Тогда первым может выучиться новый символ `r·`, за ним `l o` превратится в `lo`. К этому новому символу приклеится `w`: `lo w` $\\to$ `low`. И так далее.\n",
        "\n",
        "Утверждается, что таким образом выучатся, во-первых, все частотные и короткие слова, а во-вторых, все значимые подслова. Например, в полученном в результате алфавите должны найтись `ly·` и `tion·`.\n",
        "\n",
        "Дальше слово можно разбить на набор подслов - и действовать, как с символами.\n",
        "\n",
        "Здесь можно найти уже предобученные эмбеддинги: [BPEmb](https://github.com/bheinzerling/bpemb).\n",
        "\n",
        "**Задание* ** Посмотреть на то, какие подслова выделились для английского. Попробовать добавить такие эмбеддинги в модель (или хотя бы описать способ, как это можно сделать)."
      ]
    },
    {
      "metadata": {
        "id": "kjO_8sJldlYs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Полносвязная сеть\n",
        "\n",
        "Вообще говоря, интересно поставить такой бейзлайн: какое качество получит полносвязная сеть, которая будет просто использовать 2-3 слова слева и 2-3 слова справа для предсказания. Для слов в начале предложения можно использовать те же паддинги.\n",
        "\n",
        "В результате батч, с которым мы имеет дело, будет иметь размерность `(batch_size, 5, max_word_len)`, а предсказывать мы будем вектор размером `(batch_size,)`. При этом `batch_size` здесь будет уже заметно больше.\n",
        "\n",
        "**Задание** Реализуйте полносвязную сеть.\n",
        "\n",
        "Вообще говоря, сейчас в принципе пытаются отказываться от рекуррентных сетей в пользу более параллелизуемым. Например, для того же pos-tagging'а в прошлом году была такая статья: [Natural Language Processing with Small Feed-Forward Networks](https://arxiv.org/abs/1708.00214)"
      ]
    },
    {
      "metadata": {
        "id": "Ftt-vjDhhqgb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dropout\n",
        "\n",
        "Вспомним, что такое dropout.\n",
        "\n",
        "По сути это умножение случайно сгенерированной маски из нулей и единиц на входной вектор (+ нормировка).\n",
        "\n",
        "Например, для слоя Dropout(p):\n",
        "\n",
        "$$m = \\frac1{1-p} \\cdot \\text{Bernouli}(1 - p)$$\n",
        "$$\\tilde h = m \\odot h $$\n",
        "\n",
        "В рекуррентных сетях долго не могли прикрутить dropout. Делать это пытались, генерируя случайную маску:   \n",
        "![A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://cdn-images-1.medium.com/max/800/1*g4Q37g7mlizEty7J1b64uw.png =x300)  \n",
        "from [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
        "\n",
        "Оказалось, правильнее делать маску фиксированную: для каждого шага должны зануляться одни и те же элементы.\n",
        "\n",
        "Для pytorch нет нормального встроенного variational dropout в LSTM. Зато есть [AWD-LSTM](https://github.com/salesforce/awd-lstm-lm).\n",
        "\n",
        "Советую посмотреть обзор разных способов применения dropout'а в рекуррентных сетях: [Dropout in Recurrent Networks — Part 1](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307) (в конце - ссылки на Part 2 и 3)."
      ]
    },
    {
      "metadata": {
        "id": "4GWpWUOK-DbZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word-level Language Model"
      ]
    },
    {
      "metadata": {
        "id": "lMkPWXFw-GbI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А теперь перейдем к языковому моделированию на словном уровне."
      ]
    },
    {
      "metadata": {
        "id": "XUbt4BUk9-T2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget --quiet -c train.txt https://raw.githubusercontent.com/yoonkim/lstm-char-cnn/master/data/ptb/train.txt\n",
        "!wget --quiet -c valid.txt https://raw.githubusercontent.com/yoonkim/lstm-char-cnn/master/data/ptb/valid.txt\n",
        "!wget --quiet -c test.txt https://raw.githubusercontent.com/yoonkim/lstm-char-cnn/master/data/ptb/test.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qHmTM6J0-KJp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Будем работать с такими файлами:"
      ]
    },
    {
      "metadata": {
        "id": "Ywyqi16X-L89",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!head train.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ESidq-oa-OjZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "На каждой строке записано предложение. Все слова приведены к нижнему регистру, низкочастные заменены на <unk>, а числа - на N.\n",
        "\n",
        "Преобразуем слова в них к набору индексов."
      ]
    },
    {
      "metadata": {
        "id": "mvDye4Ti-QdD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, ):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize('train.txt')\n",
        "        self.valid = self.tokenize('valid.txt')\n",
        "        self.test = self.tokenize('test.txt')\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids\n",
        "      \n",
        "corpus = Corpus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vqznlOrt-Sm_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Начнём с бейзлайна: постройте простую N-граммную модель. \n",
        "\n",
        "Это можно сделать двумя способами: во-первых, с использованием стандартного `Counter` запомнить все встречающиеся в тексте N-граммы. При этом стоит добавлять паддинги - символы пустых слов в начале и конце предложений.\n",
        "\n",
        "Второй вариант - полносвязная нейронная сеть, которая принимает индексы N слов, и предсказывает по ним (N+1)-ое. Тут вам сразу должен вспомниться CBOW с позапрошлого занятия. Собственно, словные эмбеддинги и начались с языкового моделирования. Вот [тут](https://ronan.collobert.com/senna/) можно посмотреть, какой была жизнь до word2vec (\"Our embeddings have been trained for about 2 months, over Wikipedia\").\n",
        "\n",
        "Реализуйте оба варианта."
      ]
    },
    {
      "metadata": {
        "id": "JrHJZ7nUkmHb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Постройте модель, аналогичную той, что предсказывала символы.\n",
        "\n",
        "Теперь модель должна быть уже заметно большего размера, чтобы реально выучить язык. Используйте хотя бы два слоя LSTM. Число слоев задается параметром `LSTM`.\n",
        "\n",
        "Также стоит использовать `dropout` в LSTM.\n",
        "\n",
        "Эмбеддинги можно обучать с нуля, а можно - использовать уже готовые. Будет неплохо, если вы попробуете оба варианта. Хотя, надо отметить, в языковом моделировании обычно используют обучаемые с нуля эмбеддинги.\n",
        "\n",
        "Даже если вы будете учить эмбеддинги с нуля, стоит обратить внимание на их инициализацию. Кажется, лучше зайдет не вариант из pytorch по умолчанию, а инициализация из равномерного распределения из диапазона `(-0.05, 0.05)`.\n",
        "\n",
        "Модель, в которой сразу есть сразу и входной эмбеддинг размера, скажем `200 x 10000` и выходной слой с такой же размерностью, получается очень уж тяжелой. Обратите внимание, что можно в качестве выходного слоя использовать тот же самый слой эмбеддингов (точнее, веса из него): [Using the Output Embedding to Improve Language Models](https://arxiv.org/abs/1608.05859).\n",
        "\n",
        "В результате, эмбеддинги лучше обучаются потому, что выходной слой вообще учится гораздо лучше входного, а также потому, что этот выходной слой обновляется на каждом мини-батче, тогда как в слое эмбеддингов обновляются лишь строки, соответствующие словам в этом мини-батче.\n",
        "\n",
        "Сравните N-граммные и рекуррентную языковые модели. Обратите внимание как на размер, так и на качество.\n",
        "\n",
        "<бесстыдная самореклама> Подробнее почитать, в том числе и про то, как меряют качество (перплексию), можно здесь: [Как научить свою нейросеть генерировать стихи](https://habrahabr.ru/post/334046/) </бесстыдная самореклама>."
      ]
    }
  ]
}