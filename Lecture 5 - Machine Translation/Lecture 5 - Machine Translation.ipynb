{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 5 - Machine Translation.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1UzbRmUAosGZbYtzAWwE2kgvrjW9q6Ybj",
          "timestamp": 1522576059698
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Np2vEhSSHBA_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from typing import List, Tuple, Dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "czQgyYjmpidh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Машинный перевод"
      ]
    },
    {
      "metadata": {
        "id": "377oOZhMpl5I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В предыдущих сериях мы смотрели на примеры использования рекуррентных нейронных сетей для классификации последовательностей (предсказание языка по фамилии), для разметки последовательностей (POS-Tagging) и генерации последовательностей.\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg =x250)\n",
        "\n",
        "Последний популярный вариант применения - seq2seq: по сути, это объединение архитектур для классификации и для генерации последовательностей.\n",
        "\n",
        "Самое известное применение seq2seq - для машинного перевода:   \n",
        "![MT](https://www.tensorflow.org/images/seq2seq/seq2seq.jpg =x500)\n",
        "From [Neural Machine Translation (seq2seq) Tutorial](https://www.tensorflow.org/tutorials/seq2seq).\n",
        "\n",
        "При обработке слов на исходном языке рекуррентная сеть кодирует его содержание в скрытом векторе. \n",
        "\n",
        "Дальше этот вектор передается в генератор текста на нужном нам языке в качестве начального скрытого состояния.\n",
        "\n",
        "Сам генератор устроен так же, как и тот генератор фамилий, который мы уже писали. Но переданное скрытое состояние с закодированным в нем смыслом исходного предложения позволяет ему генерировать текст, имеющий смысл, близкий к нужному."
      ]
    },
    {
      "metadata": {
        "id": "EW5-OkhOJQmp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Данные"
      ]
    },
    {
      "metadata": {
        "id": "2SOCpDYkIKz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\"Взрослые\" датасеты можно взять здесь: http://www.statmt.org/wmt16/translation-task.html#download\n",
        "\n",
        "Мы будем использовать \"игрушечный\" датасет для en-ru: http://www.manythings.org/anki/rus-eng.zip"
      ]
    },
    {
      "metadata": {
        "id": "LRFspS_TJkGq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.manythings.org/anki/rus-eng.zip \n",
        "!unzip rus-eng.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67BdIfpjJnXL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!head rus.txt\n",
        "!wc -l rus.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QlW2weBzOBKw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!shuf rus.txt > rus.shuffled.txt\n",
        "!head rus.shuffled.txt -n 30000 > rus.val.txt\n",
        "!tail rus.shuffled.txt -n+30000 > rus.train.txt\n",
        "!cut -f 1 rus.val.txt > en.val.txt\n",
        "!cut -f 2 rus.val.txt > ru.val.txt\n",
        "!cut -f 1 rus.train.txt > en.train.txt\n",
        "!cut -f 2 rus.train.txt > ru.train.txt\n",
        "\n",
        "!head ru.train.txt\n",
        "!head en.train.txt\n",
        "!wc -l en.train.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXtdagwiIkE7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Препроцессинг"
      ]
    },
    {
      "metadata": {
        "id": "r4SRW43Lv40Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Начнем с токенизации."
      ]
    },
    {
      "metadata": {
        "id": "hiArY2GZUqIE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_file(input_file_name, output_file_name):\n",
        "    with open(input_file_name, \"r\", encoding='utf-8') as i, open(output_file_name, \"w\", encoding='utf-8') as o:\n",
        "        for line in i:\n",
        "            tokens = [token.lower() for token in word_tokenize(line.strip())]\n",
        "            o.write(\" \".join(tokens) + \"\\n\")\n",
        "\n",
        "preprocess_file(\"en.val.txt\", \"en.val.clean.txt\")\n",
        "preprocess_file(\"ru.val.txt\", \"ru.val.clean.txt\")\n",
        "preprocess_file(\"en.train.txt\", \"en.train.clean.txt\")\n",
        "preprocess_file(\"ru.train.txt\", \"ru.train.clean.txt\")\n",
        "!head \"en.val.clean.txt\"\n",
        "!head \"ru.val.clean.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q6GNGs_6wG9w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Составим пару словарей - для исходного и результирующего языка."
      ]
    },
    {
      "metadata": {
        "id": "khNMFvFELrFL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, language):\n",
        "        self.language = language\n",
        "        self.word2index = {}\n",
        "        self.word2count = Counter()\n",
        "        self.special_tokens = (\"<pad>\", \"</b>\", \"</s>\", \"<unk>\")\n",
        "        self.index2word = list(self.special_tokens)\n",
        "\n",
        "    def get_pad(self):\n",
        "        return self.index2word.index(\"<pad>\")\n",
        "\n",
        "    def get_sos(self):\n",
        "        return self.index2word.index(\"</b>\")\n",
        "\n",
        "    def get_eos(self):\n",
        "        return self.index2word.index(\"</s>\")\n",
        "\n",
        "    def get_unk(self):\n",
        "        return self.index2word.index(\"<unk>\")\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            if word == '':\n",
        "                continue\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = len(self.index2word)\n",
        "            self.word2count[word] += 1\n",
        "            self.index2word.append(word)\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def get_index(self, word):\n",
        "        if word in self.word2index:\n",
        "            return self.word2index[word]\n",
        "        else:\n",
        "            return self.get_unk()\n",
        "          \n",
        "    def get_word(self, index):\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.index2word)\n",
        "\n",
        "    def is_empty(self):\n",
        "        return self.size() <= len(self.special_tokens)\n",
        "\n",
        "    def shrink(self, n):\n",
        "        best_words = self.word2count.most_common(n)\n",
        "        self.index2word = list(self.special_tokens)\n",
        "        self.word2index = {}\n",
        "        self.word2count = Counter()\n",
        "        for word, count in best_words:\n",
        "            self.add_word(word)\n",
        "            self.word2count[word] = count\n",
        "\n",
        "    def reset(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = Counter()\n",
        "        self.index2word = list(self.special_tokens)\n",
        "\n",
        "    def save(self, path) -> None:\n",
        "        with open(path, \"wb\") as f:\n",
        "            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def load(self, path):\n",
        "        with open(path, \"rb\") as f:\n",
        "            vocab = pickle.load(f)\n",
        "            self.__dict__.update(vocab.__dict__)\n",
        "            \n",
        "    def get_indices(self, sentence: str) -> List[int]:\n",
        "        return [self.get_index(word) for word in sentence.strip().split()] + [self.get_eos()]\n",
        "      \n",
        "    def pad_indices(self, indices: List[int], max_length: int):\n",
        "        return indices + [self.get_pad() for _ in range(max_length - len(indices))]\n",
        "    \n",
        "    def add_file(self, filename: str):\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as r:\n",
        "            for line in r:\n",
        "                for word in line.strip().split():\n",
        "                    self.add_word(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AD3NhBEdQIBa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def collect_vocabularies(src_filename, src_lang, tgt_filename, tgt_lang):\n",
        "    src_vocabulary = Vocabulary(src_lang)\n",
        "    tgt_vocabulary = Vocabulary(tgt_lang)\n",
        "    src_vocabulary.add_file(src_filename)\n",
        "    src_vocabulary.save(\"src_vocabulary.pickle\")\n",
        "    tgt_vocabulary.add_file(tgt_filename)\n",
        "    tgt_vocabulary.save(\"tgt_vocabulary.pickle\")\n",
        "    return src_vocabulary, tgt_vocabulary\n",
        "  \n",
        "SRC_FILENAME = \"ru.train.clean.txt\"\n",
        "TGT_FILENAME = \"en.train.clean.txt\"\n",
        "SRC_VAL_FILENAME = \"ru.val.clean.txt\"\n",
        "TGT_VAL_FILENAME = \"en.val.clean.txt\"\n",
        "src_vocabulary, tgt_vocabulary = collect_vocabularies(SRC_FILENAME, \"en\", TGT_FILENAME, \"fr\")\n",
        "print(src_vocabulary.size(), tgt_vocabulary.size())\n",
        "\n",
        "src_vocabulary.shrink(20000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwirQLoowe1M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Напишем генератор батчей."
      ]
    },
    {
      "metadata": {
        "id": "_JsXSNmnIKPl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class BatchGenerator:\n",
        "    def __init__(self, \n",
        "                 pair_file_names: List[Tuple[str, str]], \n",
        "                 max_len: int, \n",
        "                 batch_size: int,\n",
        "                 src_vocabulary: Vocabulary, \n",
        "                 tgt_vocabulary: Vocabulary):\n",
        "        self.pair_file_names = pair_file_names  # type: List[Tuple[str, str]]\n",
        "        self.max_len = max_len  # type: int\n",
        "        self.src_vocabulary = src_vocabulary  # type: Vocabulary\n",
        "        self.tgt_vocabulary = tgt_vocabulary  # type: Vocabulary\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "    def __iter__(self):\n",
        "        batch_count = 0\n",
        "        pairs = list()\n",
        "        for src_file_name, tgt_file_name in self.pair_file_names:\n",
        "            with open(src_file_name, \"r\", encoding='utf-8') as src, open(tgt_file_name, \"r\", encoding='utf-8') as tgt:\n",
        "                for src_sentence, tgt_sentence in zip(src, tgt):\n",
        "                    src_indices = self.src_vocabulary.get_indices(src_sentence)\n",
        "                    tgt_indices = self.tgt_vocabulary.get_indices(tgt_sentence)\n",
        "                    if len(src_indices) > self.max_len or len(tgt_indices) > self.max_len:\n",
        "                        continue\n",
        "                    pairs.append((src_indices, tgt_indices))\n",
        "                    if len(pairs) == self.batch_size:\n",
        "                        src_seqs, tgt_seqs = zip(*pairs)\n",
        "                        src_batch, tgt_batch = self.process(src_seqs, self.src_vocabulary), \\\n",
        "                                               self.process(tgt_seqs, self.tgt_vocabulary)\n",
        "                        yield src_batch, tgt_batch\n",
        "                        pairs = list()\n",
        "            if len(pairs) == 0:\n",
        "                continue;\n",
        "            src_seqs, tgt_seqs = zip(*pairs)\n",
        "            src_batch, tgt_batch = self.process(src_seqs, self.src_vocabulary), \\\n",
        "                                   self.process(tgt_seqs, self.tgt_vocabulary)\n",
        "            yield src_batch, tgt_batch\n",
        "\n",
        "    def process(self, sequences: List[List[int]], vocabulary: Vocabulary):\n",
        "        lengths = BatchGenerator.get_lengths(sequences)\n",
        "        sequences = self.pad(sequences, lengths, vocabulary)\n",
        "        variable = BatchGenerator.get_variable(sequences)\n",
        "        return variable\n",
        "    \n",
        "    def pad(self, sequences: List[List[int]], lengths: List[int], vocabulary: Vocabulary):\n",
        "        return [vocabulary.pad_indices(indices, max(lengths)) for indices in sequences]\n",
        "      \n",
        "    @staticmethod\n",
        "    def get_lengths(sequences: List[List[int]]):\n",
        "        return [len(indices) for indices in sequences]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_variable(sequences: List[List[int]]):\n",
        "        return autograd.Variable(torch.LongTensor(sequences), requires_grad=False).transpose(0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QhnC75X4Rr4j",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "batch_generator = BatchGenerator([(SRC_FILENAME, TGT_FILENAME)], 10, 512, src_vocabulary, tgt_vocabulary)\n",
        "next(iter(batch_generator))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LG1_KMrLi_Yv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "val_batch_generator = BatchGenerator([(SRC_VAL_FILENAME, TGT_VAL_FILENAME)], 10, 512, src_vocabulary, tgt_vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V08q7SHmvg5n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq"
      ]
    },
    {
      "metadata": {
        "id": "_i9k4L4jwiln",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Сама модель повторяет уже описанную выше.\n",
        "\n",
        "Составим её из нескольких кусков. Во-первых, `EncoderRNN` будет читать предложение на исходном языке и строить его вектор смысла.\n",
        "\n",
        "**Задание** Реализуйте метод `forward`. Обратите внимание, самое главное, что он возвращает - это `hidden`, а не `outputs`, как обычно."
      ]
    },
    {
      "metadata": {
        "id": "lSEiSFdtwosT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_dim, hidden_size, n_layers=3, dropout=0.3):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, input_seqs, hidden=None):\n",
        "        <your code here>\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BqEsqwuRxMz0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Декодер будет устроен сложнее.\n",
        "\n",
        "На вход рекуррентной сети внутри него на каждом шаге нужно отдавать предыдущее слово.\n",
        "\n",
        "Различают два способа, откуда это предыдущее слово может взяться: во-первых, *teacher forcing* - это когда у нас есть предложение, которое мы должны сгенерировать и для генерации следующего слова используется предыдущее слово из этого правильного варианта. В итоге, сеть могла сгенерировать неправильное слово на прошлом шаге - но мы, как строгий учитель, исправляем её вариант на правильный.\n",
        "\n",
        "Альтернативный вариант - это использовать то слово, которое сгенерировала сеть. Если использовать teacher forcing сеть становится более зависимой от правильности предыдущего слова - она просто привыкает, что предыдущее слово всегда правильное. Поэтому тренировать сеть в условиях, близких к боевым, может быть очень полезным.\n",
        "\n",
        "Pytorch предоставляет очень удобную возможность сочетать оба подхода: почему бы не тренировать сеть иногда с teacher forcing, иногда - без. Можно случайно выбирать метод для каждого мини-батча - скажем, с 50%-ой вероятностью каждый из вариантов.\n",
        "\n",
        "Чтобы это работало, добавим в `forward` декодера параметр `gtruth`, который как раз и будет содержать (или не содержать) правильное предложение.\n",
        "\n",
        "**Задание** Реализуйте класс `Generator`, который будет просто проецировать скрытое состояние декодера на словарь и строить распределение вероятностей предсказаний слов.\n",
        "\n",
        "Реализуйте метод `step` в декодере, который будет делать один шаг рекуррентной сети (с новым только сгенерированным словом).\n",
        "\n",
        "Реализуйте сэмплирование из предсказания генератора для случая обучения без teacher forcing'а."
      ]
    },
    {
      "metadata": {
        "id": "HNkA0aJexMH_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.sm = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        assert inputs.size(1) == self.hidden_size\n",
        "        <your code here>\n",
        "        return output\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_size, output_size, max_length, n_layers=3, \n",
        "                 dropout=0.3, use_cuda=False):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda = use_cuda\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout)\n",
        "        self.generator = Generator(hidden_size, output_size)\n",
        "        \n",
        "    def step(self, batch_input, hidden):       \n",
        "        <your code here>\n",
        "        return output, hidden\n",
        "\n",
        "    def init_state(self, batch_size, sos_index):\n",
        "        initial_input = autograd.Variable(torch.zeros((batch_size, )).type(torch.LongTensor))\n",
        "        initial_input = torch.add(initial_input, sos_index)\n",
        "        initial_input = initial_input.cuda() if self.use_cuda else initial_input\n",
        "        return initial_input\n",
        "\n",
        "    def forward(self, current_input, hidden, length, gtruth=None):\n",
        "        batch_size = current_input.size(0)\n",
        "        outputs = autograd.Variable(torch.zeros(length, batch_size, self.output_size))\n",
        "        outputs = outputs.cuda() if self.use_cuda else outputs\n",
        "\n",
        "        for t in range(length):\n",
        "            output, hidden = self.step(current_input, hidden)\n",
        "            scores = self.generator.forward(output.squeeze(0))\n",
        "            outputs[t] = scores\n",
        "            if gtruth is None:\n",
        "                <generate next word>\n",
        "                current_input = top_indices\n",
        "            else:\n",
        "                current_input = gtruth[t]\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3caP7Xjo0xxl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Сам класс `Seq2Seq` будет просто прятать внутри себя энкодер и декодер, запуская их поочереди:"
      ]
    },
    {
      "metadata": {
        "id": "9ki1hX_aV3jl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, embedding_dim, rnn_size, input_size, output_size, encoder_n_layers, decoder_n_layers, dropout,\n",
        "                 max_length, use_cuda):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.rnn_size = rnn_size\n",
        "        self.encoder_n_layers = encoder_n_layers\n",
        "        self.decoder_n_layers = decoder_n_layers\n",
        "        self.dropout = dropout\n",
        "        self.max_length = max_length\n",
        "        self.use_cuda = use_cuda\n",
        "        \n",
        "        self.encoder = EncoderRNN(self.input_size, embedding_dim, rnn_size, dropout=dropout,\n",
        "                                  n_layers=encoder_n_layers)\n",
        "        self.decoder = DecoderRNN(embedding_dim, rnn_size, self.output_size, dropout=dropout,\n",
        "                                  max_length=max_length, n_layers=decoder_n_layers, use_cuda=use_cuda)\n",
        "\n",
        "    def forward(self, variable, sos_index, gtruth=None):\n",
        "        encoder_output, encoder_hidden = self.encoder.forward(variable)\n",
        "        current_input = self.decoder.init_state(variable.size(1), sos_index)\n",
        "        max_length = self.max_length\n",
        "        if gtruth is not None:\n",
        "            max_length = min(self.max_length, gtruth.size(0))\n",
        "        decoder_output, _ = self.decoder.forward(current_input, encoder_hidden, max_length, gtruth)\n",
        "\n",
        "        return encoder_output, decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y8pEtwGOX0nH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "model = Seq2Seq(embedding_dim=200, \n",
        "                rnn_size=200, \n",
        "                input_size=src_vocabulary.size(),\n",
        "                output_size=tgt_vocabulary.size(), \n",
        "                encoder_n_layers=2, \n",
        "                decoder_n_layers=2, \n",
        "                dropout=0.3, \n",
        "                max_length=10, \n",
        "                use_cuda=use_cuda)\n",
        "model = model.cuda() if use_cuda else model\n",
        "print(model)\n",
        "params = sum([np.prod(p.size()) for p in model.parameters()])\n",
        "print(\"Params: \", params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hSxDfPl53fdY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Допишите функцию для обучения модели."
      ]
    },
    {
      "metadata": {
        "id": "Kn94Dh70TaO4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "main_optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "weight = torch.ones(tgt_vocabulary.size())\n",
        "weight[tgt_vocabulary.get_pad()] = 0\n",
        "weight = weight.cuda() if use_cuda else weight\n",
        "criterion = nn.NLLLoss(weight, size_average=False)\n",
        "\n",
        "print_every = 100\n",
        "loss_average = 0\n",
        "big_epochs = 3\n",
        "for big_epoch in range(big_epochs):\n",
        "    for epoch, (src_batch, tgt_batch) in enumerate(batch_generator):\n",
        "        src_batch = src_batch.cuda() if use_cuda else src_batch\n",
        "        tgt_batch = tgt_batch.cuda() if use_cuda else tgt_batch\n",
        "        <your code here>\n",
        "    loss_average = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "27xUd-cs5KKc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Перевод для простоты будет делать так: на каждом шаге выбирать слово с наибольшей вероятностью.\n",
        "\n",
        "При этом более правильным будет использовать beam search: на каждом шаге выбирать несколько вариантов продолжений текущих лучших последовательностей:  \n",
        "![](https://4.bp.blogspot.com/-Jjpb7iyB37A/WBZI4ImGQII/AAAAAAAAA9s/ululnUWt2vw9NMKuEr-F9H8tR2LEv36lACLcB/s1600/prefix_probability_tree.png)   \n",
        "From [Using beam search to generate the most probable sentence](https://geekyisawesome.blogspot.ru/2016/10/using-beam-search-to-generate-most.html)."
      ]
    },
    {
      "metadata": {
        "id": "FKrWFo2thNpP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def translate(model, sentence, src_vocabulary, tgt_vocabulary):\n",
        "    model.eval()\n",
        "    tokens = [token.lower() for token in word_tokenize(sentence.strip())]\n",
        "    indices = src_vocabulary.get_indices(\" \".join(tokens))\n",
        "    variable = autograd.Variable(torch.LongTensor(indices))\n",
        "    variable = variable.unsqueeze(1)\n",
        "    variable = variable.cuda() if use_cuda else variable\n",
        "    _, output = model.forward(variable, tgt_vocabulary.get_sos())\n",
        "    result = []\n",
        "    for t in range(output.size(0)):\n",
        "        top_indices = output[t].topk(1, dim=1)[1].view(-1)\n",
        "        index = top_indices.data[0]\n",
        "        if index == tgt_vocabulary.get_eos():\n",
        "            break\n",
        "        result.append(tgt_vocabulary.get_word(index))\n",
        "    return \" \".join(result)\n",
        "\n",
        "translate(model, \"Тебя зовут Том.\", src_vocabulary, tgt_vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aLch-s4q8JOY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Стандартным способом оценки качества модели является [BLEU метрика](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)."
      ]
    },
    {
      "metadata": {
        "id": "yWNctG3WiN97",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xO2oIFG6kjcR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with open(SRC_VAL_FILENAME, \"r\", encoding='utf-8') as f, open(\"pred.txt\", \"w\", encoding='utf-8') as w:\n",
        "    i = 0\n",
        "    for line in f:\n",
        "        w.write(translate(model, line, src_vocabulary, tgt_vocabulary)+\"\\n\")\n",
        "        if i % 500 == 0:\n",
        "            print(i)\n",
        "        i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkuIySIAkeN0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!perl multi-bleu.perl en.val.clean.txt < pred.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01MddvEPmmaw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ]
    },
    {
      "metadata": {
        "id": "1JfwTWx6sb4r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Одна из основных проблем того, что мы делали до сих пор хорошо формулируется так:\n",
        "```\n",
        "\"You can't cram the meaning of a whole %&!$ing sentence into a single $&!*ing vector!\"\n",
        "(c) Prof. Ray Mooney\n",
        "```\n",
        "\n",
        "Давайте запоминать все скрытые состояния энкодера, а не только последнее.\n",
        "\n",
        "Дальше, для вычисления нового слова при генерации найдем сначала представление уже сгенерированного контекста (по которому обычно и генерируется следующее слово).  \n",
        "По этому представлению посчитаем оценки полезности состояний энкодера: `attention weights` на картинке ниже. Чем выше вес - тем более полезно состояние. (Можно, кстати, представлять, что в предыдущем варианте мы просто давали всем состояниям кроме последнего вес 0, а последнему - 1).\n",
        "\n",
        "С этими весами состояния энкодера суммируются, и мы получаем взвешенный вектор-представление контекста. Опять вектор?! Но теперь этот вектор получен для конкретного генерируемого слова - это же гораздо лучше, чем пытаться сделать один вектор сразу для всех генерируемых слов.\n",
        "\n",
        "![attention](https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg =x400)  \n",
        "From [Neural Machine Translation (seq2seq) Tutorial](https://www.tensorflow.org/tutorials/seq2seq).\n",
        "\n",
        "В результате получаются такие красивые картинки с визуализацией аттеншена:   \n",
        "![att-vis](https://www.tensorflow.org/images/seq2seq/attention_vis.jpg =x500)\n",
        "\n",
        "Яркость ячейки показывает насколько много внимания уделяла модель данному слову на исходном языке при генерации соответствующего ему слова.\n",
        "\n",
        "Очень красивая статья с демонстрацией attention'а: [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/).\n",
        "\n",
        "---\n",
        "\n",
        "В общем случае, attention работает так: пусть у нас есть набор скрытых состояний $\\mathbf{s}_1, \\ldots, \\mathbf{s}_m$ - представлений слов из исходного языка, полученных с помощью энкодера. И есть некоторое текущее скрытое состояние $\\mathbf{h}_i$ - скажем, представление, используемое для предсказания слова на нужном нам языке.\n",
        "\n",
        "Тогда с помощью аттеншена мы можем получить взвешенное представление контекста $\\mathbf{s}_1, \\ldots, \\mathbf{s}_m$ - вектор $c_i$:\n",
        "$$\n",
        "\\begin{align}\\begin{split}\n",
        "\\mathbf{c}_i &= \\sum\\limits_j a_{ij}\\mathbf{s}_j\\\\\n",
        "\\mathbf{a}_{ij} &= \\text{softmax}(f_{att}(\\mathbf{h}_i, \\mathbf{s}_j))\n",
        "\\end{split}\\end{align}\n",
        "$$\n",
        "\n",
        "$f_{att}$ - функция, которая говорит, насколько хорошо $\\mathbf{h}_i$ и $\\mathbf{s}_j$ подходят друг другу.\n",
        "\n",
        "Самые популярные её варианты:\n",
        "- Additive attention:\n",
        "$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{v}_a{}^\\top \\text{tanh}(\\mathbf{W}_a[\\mathbf{h}_i; \\mathbf{s}_j])$$\n",
        "- Dot attention:\n",
        "$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{h}_i^\\top \\mathbf{s}_j$$\n",
        "- Multiplicative attention:\n",
        "$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{h}_i^\\top \\mathbf{W}_a \\mathbf{s}_j$$"
      ]
    },
    {
      "metadata": {
        "id": "LHLbUIFtGwgN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "Напишем реализацию простенького attention'а."
      ]
    },
    {
      "metadata": {
        "id": "-SAsJcCpmmDe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_size, output_size, max_length, n_layers=3, \n",
        "                 dropout=0.3, use_cuda=False, use_attention=True):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda = use_cuda\n",
        "        self.max_length = max_length\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
        "        \n",
        "        if self.use_attention:\n",
        "            self.attn = nn.Linear(hidden_size + embedding_dim, self.max_length, bias=False)\n",
        "            self.attn_sm = nn.Softmax(dim=1)\n",
        "            self.attn_out = nn.Linear(hidden_size + embedding_dim, embedding_dim, bias=False)\n",
        "            self.attn_out_relu = nn.ReLU()\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout)\n",
        "        self.generator = Generator(hidden_size, output_size)\n",
        "        \n",
        "    def step(self, batch_input, hidden, encoder_output):\n",
        "        # batch_input: B\n",
        "        # hidden: (n_layers x B x N, n_layers x B x N)\n",
        "        # encoder_output: L x B x N\n",
        "        # output: 1 x B x N\n",
        "        # embedded:  B x E\n",
        "        # attn_weights: B x 1 x L\n",
        "        # context: B x 1 x N\n",
        "        # rnn_input: B x N\n",
        "        \n",
        "        embedded = self.embedding(batch_input)\n",
        "        \n",
        "        attn_weights = None\n",
        "        if self.use_attention:\n",
        "            attn_weights = self.attn_sm(self.attn(torch.cat((embedded, hidden[0][-1]), 1))).unsqueeze(1)\n",
        "            max_length = encoder_output.size(0)\n",
        "            context = torch.bmm(attn_weights[:, :, :max_length], encoder_output.transpose(0, 1))\n",
        "            rnn_input = torch.cat((embedded, context.squeeze(1)), 1)\n",
        "            rnn_input = self.attn_out_relu(self.attn_out(rnn_input))\n",
        "        else:\n",
        "            rnn_input = embedded\n",
        "        output, hidden = self.rnn(rnn_input.unsqueeze(0), hidden)\n",
        "        return output, hidden, attn_weights[:, :, :max_length].transpose(0, 1).squeeze(0)\n",
        "\n",
        "    def init_state(self, batch_size, sos_index):\n",
        "        initial_input = autograd.Variable(torch.zeros((batch_size,)).type(torch.LongTensor))\n",
        "        initial_input = torch.add(initial_input, sos_index)\n",
        "        initial_input = initial_input.cuda() if self.use_cuda else initial_input\n",
        "        return initial_input\n",
        "\n",
        "    def forward(self, current_input, hidden, length, encoder_output, gtruth=None):\n",
        "        outputs = autograd.Variable(torch.zeros(length, current_input.size(0), self.output_size))\n",
        "        outputs = outputs.cuda() if self.use_cuda else outputs\n",
        "        \n",
        "        attn_outputs = autograd.Variable(torch.zeros(length, current_input.size(0), encoder_output.size(0)))\n",
        "        attn_outputs = attn_outputs.cuda() if self.use_cuda else attn_outputs\n",
        "\n",
        "        for t in range(length):\n",
        "            output, hidden, attn_weights = self.step(current_input, hidden, encoder_output)\n",
        "            attn_outputs[t] = attn_weights\n",
        "            scores = self.generator.forward(output.squeeze(0))\n",
        "            outputs[t] = scores\n",
        "            if gtruth is None:\n",
        "                <your code here>\n",
        "                current_input = top_indices\n",
        "            else:\n",
        "                current_input = gtruth[t]\n",
        "        return outputs, hidden, attn_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gfreh3Rjm3Yu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class AttnSeq2Seq(nn.Module):\n",
        "    def __init__(self, embedding_dim, rnn_size, input_size, output_size, encoder_n_layers, decoder_n_layers, dropout,\n",
        "                 max_length, use_cuda, bidirectional, use_attention=True):\n",
        "        super(AttnSeq2Seq, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.rnn_size = rnn_size\n",
        "        self.encoder_n_layers = encoder_n_layers\n",
        "        self.decoder_n_layers = decoder_n_layers\n",
        "        self.dropout = dropout\n",
        "        self.max_length = max_length\n",
        "        self.use_cuda = use_cuda\n",
        "        self.bidirectional = bidirectional\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        self.encoder = EncoderRNN(self.input_size, embedding_dim, rnn_size, dropout=dropout,\n",
        "                                  n_layers=encoder_n_layers, bidirectional=bidirectional)\n",
        "        self.decoder = AttnDecoderRNN(embedding_dim, rnn_size, self.output_size, dropout=dropout,\n",
        "                                      max_length=max_length, n_layers=decoder_n_layers, use_cuda=use_cuda, \n",
        "                                      use_attention=use_attention)\n",
        "\n",
        "    def forward(self, variable, sos_index, gtruth=None):\n",
        "        encoder_output, encoder_hidden = self.encoder.forward(variable)\n",
        "        current_input = self.decoder.init_state(variable.size(1), sos_index)\n",
        "        max_length = self.max_length\n",
        "        if gtruth is not None:\n",
        "            max_length = min(self.max_length, gtruth.size(0))\n",
        "        decoder_output, _, attn_output = \\\n",
        "            self.decoder.forward(current_input, encoder_hidden, \n",
        "                                 max_length, encoder_output, gtruth)\n",
        "\n",
        "        return encoder_output, decoder_output, attn_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lvJE5wOor8md",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "model = AttnSeq2Seq(embedding_dim=300, \n",
        "                rnn_size=300, \n",
        "                input_size=src_vocabulary.size(),\n",
        "                output_size=tgt_vocabulary.size(), \n",
        "                encoder_n_layers=2, \n",
        "                decoder_n_layers=2, \n",
        "                dropout=0.3, \n",
        "                max_length=10, \n",
        "                use_cuda=use_cuda, \n",
        "                bidirectional=False)\n",
        "model = model.cuda() if use_cuda else model\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KxTc78Ir1E2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "main_optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "weight = torch.ones(tgt_vocabulary.size())\n",
        "weight[tgt_vocabulary.get_pad()] = 0\n",
        "weight = weight.cuda() if use_cuda else weight\n",
        "criterion = nn.NLLLoss(weight, size_average=False)\n",
        "print_every = 100\n",
        "loss_average = 0\n",
        "big_epochs = 3\n",
        "for big_epoch in range(big_epochs):\n",
        "    for epoch, (src_batch, tgt_batch) in enumerate(batch_generator):\n",
        "        src_batch = src_batch.cuda() if use_cuda else src_batch\n",
        "        tgt_batch = tgt_batch.cuda() if use_cuda else tgt_batch\n",
        "        <your code here>\n",
        "    loss_average = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pqdjVHM2GoRf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Попробуем тоже визуализировать attention!"
      ]
    },
    {
      "metadata": {
        "id": "-VgRiBWxubv_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def show_attention(input_tokens, output_tokens, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.cpu().data.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([\"\"] + input_tokens, rotation=90)\n",
        "    ax.set_yticklabels([\"\"] + output_tokens)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LOgXsi3KtKTt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def translate(model, sentence, src_vocabulary, tgt_vocabulary):\n",
        "    model.eval()\n",
        "    tokens = [token.lower() for token in word_tokenize(sentence.strip())]\n",
        "    indices = src_vocabulary.get_indices(\" \".join(tokens))\n",
        "    variable = autograd.Variable(torch.LongTensor(indices))\n",
        "    variable = variable.unsqueeze(1)\n",
        "    variable = variable.cuda() if use_cuda else variable\n",
        "    _, output, attn = model.forward(variable, tgt_vocabulary.get_sos())\n",
        "    attn = attn.squeeze(1)\n",
        "    result = []\n",
        "    for t in range(output.size(0)):\n",
        "        top_indices = output[t].topk(1, dim=1)[1].view(-1)\n",
        "        index = top_indices.data[0]\n",
        "        if index == tgt_vocabulary.get_eos():\n",
        "            break\n",
        "        result.append(tgt_vocabulary.get_word(index))\n",
        "    show_attention(tokens, result, attn)\n",
        "    return \" \".join(result)\n",
        "\n",
        "translate(model, \"Тебя зовут Том.\", src_vocabulary, tgt_vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jL8iU9ayHG4y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте Additive attention и Multiplicative attention."
      ]
    }
  ]
}