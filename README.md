# Deep NLP Course at ABBYY

The crash course to the deep learning for NLP at ABBYY.

The textbook: [Neural Network Methods in Natural Language Processing by Yoav Goldberg](https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984)

## Lecture Materials
### Lecture 1: *Introduction*  
A short overview of the most popular architectures in the deep learning.   
A brief introduction into the [keras](keras.io) framework.  
Examples of models for sentiment analysis on the IMDB movie review dataset.  
The [Colab notebook](https://colab.research.google.com/notebook#fileId=1KGy9Hm3y4asE6ohg3QD77w2nZV3V9_08).  
[Additional materials and deadlines](https://github.com/DanAnastasyev/DeepNLP-Course/tree/master/Lecture%201%20-%20Introduction).

### Lecture 2: *Fully-Connected Neural Networks and Word2Vec Models*  
An introduction into the [PyTorch](pytorch.org) framework.   
Example of a simple bag-of-words model for the sentiment analysis.  
Examples of Word2Vec models: CBOW, Skip-Gram and Negative Sampling.  
The [Colab notebook](https://colab.research.google.com/drive/15tg6jTt1F0oR5PzFlcZNBnpPiu4se1m3).  
[Additional materials and deadlines](https://github.com/DanAnastasyev/DeepNLP-Course/tree/master/Lecture%202%20-%20PyTorch%20%26%20Word2Vec).

### Lecture 3: *Recurrent Neural Networks, part 1*  
A simple implementation of the vanilla RNN and an illustration of the vanishing gradient problem.  
Examples of character-level LSTMs for:
- Text classification: language prediction by the surname's spelling;
- Conditional text generation: generation of the surname given language.  

The [Colab notebook](https://colab.research.google.com/drive/1mLUuEcEBAqw8WwlezEgkXhjeulOAE5fW).  
[Additional materials and deadlines](https://github.com/DanAnastasyev/DeepNLP-Course/tree/master/Lecture%203%20-%20RNNs).

### Lecture 4: *Recurrent Neural Networks, part 2*  
An implementation of a character-level BiLSTM POS-tagger.  
An overview of word-level language models: N-gram, fully-connected and RNN ones.  
The [Colab notebook](https://colab.research.google.com/drive/1CN-OhE-mbYstkxyJ24NFcNy8SpYMAeM0).  
[Additional materials and deadlines](https://github.com/DanAnastasyev/DeepNLP-Course/tree/master/Lecture%204%20-%20RNNs%2C%20part%202).

### Lecture 5: *Machine Translation*  
An implementation of simple Seq2Seq model for machine translation.  
An example of Attention-based machine translation model.   
The [Colab notebook](https://colab.research.google.com/drive/1ZkxDQUcoc0LUjjiP6nlQwYuBPWLxZFZD).  
[Additional materials and deadlines](https://github.com/DanAnastasyev/DeepNLP-Course/tree/master/Lecture%205%20-%20Machine%20Translation).

### Lecture 6: *Convolutional Neural Networks and Transformers*  
An example of character-level convolutional neural network for word classification.  
An explanation of similarities and difference between convolutions and attention.  
An overview of Transformer architecture.    
The [Colab notebook](https://colab.research.google.com/drive/1W5txH-ssmSSN7WEfP2iAkl7zHAgMQUcN).  
[Additional materials and deadlines](https://github.com/DanAnastasyev/DeepNLP-Course/tree/master/Lecture%206%20-%20Convolutional%20Neural%20Networks%20%26%20Transformers).

---
[A questionnaire about the course](https://goo.gl/forms/7I9QUbc9FJwl2sNl1)
