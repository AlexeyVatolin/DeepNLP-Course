{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 3 - RNNs.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YKoTq9xW-PdW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl torchvision\n",
        "!pip install -q keras\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.cuda import FloatTensor, LongTensor\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Q5wMjxQMeQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Рекуррентные нейронные сети"
      ]
    },
    {
      "metadata": {
        "id": "qo0lsnzV-i-P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Simple RNN"
      ]
    },
    {
      "metadata": {
        "id": "Hb4_VaBIMVFD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Вообще говоря, я предполагаю, что вы уже несколько раз слышали про RNN.\n",
        "\n",
        "Основная их прелесть - расшаренные параметры. Посмотрите на картинку:\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg =x250)\n",
        "\n",
        "[(from The Unreasonable Effectiveness of Recurrent Neural Networks)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) \n",
        "\n",
        "Первый пример - это обычная полносвязная сеть. Каждый следующий демонстрирует обработку некоторой последовательности произвольной длины (красные прямоугольнички) и генерацию выходной последовательности, также произвольной длины (синие прямоугольники).\n",
        "\n",
        "При этом зеленые прямоугольники в каждом рисунке - это одни и те же веса. Получается, мы, с одной стороны, обучаем очень-очень глубокую сеть (если посмотреть на неё перевернутую), а с другой - строго ограниченное количество параметров.\n",
        "\n",
        "---\n",
        "Напишем сразу простую RNN!\n",
        "\n",
        "Напомню, делает она примерно вот это:\n",
        "\n",
        "![rnn-unrolled](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png =x220)\n",
        "\n",
        "[(from Understanding LSTM Networks)](http://colah.github.io/posts/2015-08-Understanding-LSTMs)\n",
        "\n",
        "Вообще говоря, можно придумать много вариаций на тему такой реализации. В нашем случае, обработка будет такой:\n",
        "$$h_t = tanh(W_h [h_{t-1}; x_t] + b_h)$$\n",
        "\n",
        "$h_{t-1}$ - скрытое состояние, полученное на предыдущем шаге, $x_t$ - входной вектор. $[h_{t-1}; x_t]$ - простая конкатенация векторов. Всё как на картинке!\n",
        "\n",
        "Будем применять ещё и линейное отображение на пространство заданной выходной размерности для получения выходного вектора $s_t$:\n",
        "$$s_t = W_o h_t + b_o$$"
      ]
    },
    {
      "metadata": {
        "id": "Boscp23L-hQ6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.hidden_linear = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.output_linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, inputs, hidden):\n",
        "        combined = torch.cat((inputs, hidden), 1)\n",
        "        \n",
        "        hidden = self.hidden_linear(combined)\n",
        "        hidden = F.tanh(hidden)\n",
        "        \n",
        "        output = self.output_linear(hidden)        \n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return Variable(torch.zeros(batch_size, self.hidden_size).cuda())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7NQ_nYfEuyL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Проверим нашу сеть на очень простой задаче: заставим её говорить индекс первого элемента в последовательности.\n",
        "\n",
        "Т.е. для последовательности `[1, 2, 1, 3]` сеть должна предсказывать `1`.\n",
        "\n",
        "Начнем с генерации батча. Батч содержит исключительно цифры, закодируем их с помощью one-hot-encoding представления."
      ]
    },
    {
      "metadata": {
        "id": "yOI4JGgHT-z3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_data(batch_size=128, seq_len=5):\n",
        "    data = np.random.randint(10, size=(seq_len, batch_size))\n",
        "\n",
        "    X = Variable(FloatTensor(to_categorical(data)))\n",
        "    y = Variable(LongTensor(data[0]))\n",
        "    return X, y\n",
        "\n",
        "X_val, y_val = generate_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQ0Gsr4SFNtB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Обратите внимание, что батч имеет размерность `(sequence_length, batch_size, input_size)`. Все `RNN` в pytorch работают с таким форматом по умолчанию. В keras эти размерности шли в другом порядке: сначала `batch_size`, потом `sequence_length`.\n",
        "\n",
        "Сделано это из соображений производительности, но при желании можно поменять такое поведение с помощью аргумента `batch_first`."
      ]
    },
    {
      "metadata": {
        "id": "IbVk7zUjUQ_v",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "rnn = SimpleRNN(input_size=10, hidden_size=32, output_size=10)\n",
        "rnn = rnn.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters())\n",
        "\n",
        "total_loss = 0\n",
        "epochs_count = 1000\n",
        "for epoch_ind in range(epochs_count):\n",
        "    X_train, y_train = generate_data()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    rnn.train()\n",
        "    hidden = rnn.init_hidden(batch_size=128)\n",
        "    for i in range(X_train.size()[0]):\n",
        "        output, hidden = rnn(X_train[i], hidden)\n",
        "\n",
        "    loss = criterion(output, y_train)\n",
        "    loss.backward()\n",
        "    \n",
        "    nn.utils.clip_grad_norm(rnn.parameters(), 1.)\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.data[0]\n",
        "    \n",
        "    if epoch_ind != 0 and epoch_ind % 100 == 0:\n",
        "        rnn.eval()\n",
        "        hidden = rnn.init_hidden(batch_size=128)\n",
        "        for i in range(X_val.size()[0]):\n",
        "            output, hidden = rnn(X_val[i], hidden)\n",
        "        val_loss = criterion(output, y_val)\n",
        "        print('[{}/{}] Train: {:6f} Val: {:6f}'.format(epoch_ind, epochs_count, \n",
        "                                                       total_loss / 100, val_loss.data[0]))\n",
        "        total_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dQJg3FROIq4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Посмотрите на то, как влияет длина последовательности на работу сети. \n",
        "\n",
        "Во-первых, посмотри, с какой длиной сеть в состоянии учиться. Во-вторых, попробуйте обучить сеть с небольшой длиной последовательности, а потом применять её к более длинным.\n",
        "\n",
        "*Почему получается то, что получается?*\n",
        "\n",
        "**Задание** Утверждается, что `relu` подходит для RNN лучше. Попробуйте и её."
      ]
    },
    {
      "metadata": {
        "id": "13x5erUgTjDC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTM и GRU\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PAjZh9YkYAMH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Если всё пошло по плану, мы должны были посмотреть на то, как RNN'ки забывают. \n",
        "\n",
        "Чтобы понять причину, стоит вспомнить, как именно происходит обучение RNN, например, здесь: [Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/) или здесь - [Vanishing Gradients & LSTMs](http://harinisuresh.com/2016/10/09/lstms/).\n",
        "\n",
        "Если кратко, одна из проблем обучения рекуррентных сетей - *взрыв градиентов*. Она проявляется, когда матрица весов такова, что увеличивает норму вектора градиента при обратном проходе. В результате норма градиента экспоненциально растет и он \"взрывается\". \n",
        "\n",
        "Эту проблему мы решили с помощью клипинга градиентов: `nn.utils.clip_grad_norm(rnn.parameters(), 1.)`.\n",
        "\n",
        "Другая проблема - *затухание градиентов*. Она связана наоборот - с экспоненциальным затуханием градиентов. И вот её решают уже более сложными способами. \n",
        "\n",
        "А именно - используют gate'овые архитектуры.\n",
        "\n",
        "Идея gate'а простая, но важная, используются они далеко не только в рекуррентных сетях.\n",
        "\n",
        "Если посмотреть на то, как работает наша SimpleRNN, можно заметить, что каждый раз память (т.е. $h_t$) перезаписывается. Хочется иметь возможность сделать эту перезапись контролируемой: не отбрасывать какую-то важную инфомацию из вектора.\n",
        "\n",
        "Заведем для этого вектор $g \\in {0,1}^n$, который будет говорить, какие ячейки $h_{t-1}$ хорошие, а вместо каких стоит подставить новые значения:\n",
        "$$h_t = g \\odot f(x_t, h_{t-1}) + (1 - g) \\odot h_{t-1}.$$\n",
        "\n",
        "Например:\n",
        "$$\n",
        " \\begin{bmatrix}\n",
        "  8 \\\\\n",
        "  11 \\\\\n",
        "  3 \\\\\n",
        "  7\n",
        " \\end{bmatrix} =\n",
        " \\begin{bmatrix}\n",
        "  0 \\\\\n",
        "  1 \\\\\n",
        "  0 \\\\\n",
        "  0\n",
        " \\end{bmatrix}\n",
        " \\odot\n",
        "  \\begin{bmatrix}\n",
        "  7 \\\\\n",
        "  11 \\\\\n",
        "  6 \\\\\n",
        "  5\n",
        " \\end{bmatrix}\n",
        " +\n",
        "  \\begin{bmatrix}\n",
        "  1 \\\\\n",
        "  0 \\\\\n",
        "  1 \\\\\n",
        "  1\n",
        " \\end{bmatrix}\n",
        " \\odot\n",
        "  \\begin{bmatrix}\n",
        "  8 \\\\\n",
        "  5 \\\\\n",
        "  3 \\\\\n",
        "  7\n",
        " \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Чтобы добиться дифференцируемости, будем использовать сигмоиду: $\\sigma(f(x_t, h_{t-1}))$.\n",
        "\n",
        "В результате сеть будет сама, глядя на входы, решать, какие ячейки своей памяти и насколько стоит перезаписывать.\n",
        "\n",
        "### LSTM\n",
        "\n",
        "Кажется, первой архитектурой, применившей данной механизм, стал LSTM (Long Short-Term Memory).\n",
        "\n",
        "В ней у нас к $h_{t-1}$ добавляется ещё и $c_{t-1}$: $h_{t-1}$ - это всё то же скрытое состояния полученное на предыдущем шаге, а $c_{t-1}$ - это вектор памяти.\n",
        "\n",
        "Для начала мы можем точно так же, как и раньше посчитать новое скрытое состояние (обозначим его $\\tilde c_{t}$):\n",
        "$$\\tilde c_{t} = tanh(W_h [h_{t-1}; x_t] + b_h)$$\n",
        "\n",
        "В обычных RNN мы бы просто перезаписали этим значением сторое скрытое состояние. А теперь мы хотим понять, насколько нам нужна информация из $c_{t-1}$ и из $\\tilde c_{t}$. \n",
        "\n",
        "Оценим её сигмоидами:\n",
        "$$f = \\sigma(W_f [h_{t-1}; x_t] + b_f),$$\n",
        "$$i = \\sigma(W_i [h_{t-1}; x_t] + b_i).$$\n",
        "\n",
        "Первая - про то, насколько хочется забыть старую информацию. Вторая - насколько интересна новая. Тогда\n",
        "$$c_t = f \\odot c_{t-1} + i \\odot \\tilde c_t.$$\n",
        "\n",
        "Новое скрытое состояние мы также взвесим:\n",
        "$$o = \\sigma(W_o [h_{t-1}; x_t] + b_o),$$\n",
        "$$h_t = o \\odot tanh(c_t).$$\n",
        "\n",
        "Настоятельно рекомендуется почитать статью: [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) для более подробного ознакомления и прикольных картинок.\n",
        "\n",
        "Зачем я выписал эти формулы? Главное - чтобы показать, насколько больше параметров нужно учить в LSTM по сравнению с обычным RNN. В четыре раза больше!"
      ]
    },
    {
      "metadata": {
        "id": "_9KWgbwQMatn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Классификация имён"
      ]
    },
    {
      "metadata": {
        "id": "RT6rMrZtTDiD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Начнём уже решать задачи. \n",
        "\n",
        "Сперва попробуем классифицировать имена: потренируем сеть символьного уровня, которая будет говорить, из какого языка пришло такое имя.\n",
        "\n",
        "Должно получаться что-то вроде:\n",
        "\n",
        "```\n",
        "Hinton\n",
        "(-0.47) Scottish\n",
        "(-1.52) English\n",
        "(-3.57) Irish\n",
        "\n",
        "Schmidhuber\n",
        "(-0.19) German\n",
        "(-2.48) Czech\n",
        "(-2.68) Dutch\n",
        "```\n",
        "\n",
        "Скачаем данные."
      ]
    },
    {
      "metadata": {
        "id": "siNBESXaTyEq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -O data.zip https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7-fQPwJUKtV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import os\n",
        "\n",
        "all_letters = string.ascii_letters + \".,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "# 0 - индекс паддинга!\n",
        "char2index = {c : i + 1 for i, c in enumerate(all_letters)}\n",
        "char2index['</s>'] = max(char2index.values()) + 1\n",
        "char2index['<unk>'] = max(char2index.values()) + 1\n",
        "index2char = {char2index[c] : c for c in char2index}\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        return [unicodeToAscii(line.strip()) for line in f]\n",
        "\n",
        "files = ['data/names/' + file for file in os.listdir('data/names') if file.endswith('.txt')]  \n",
        "\n",
        "for filename in files:\n",
        "    category = filename.split('/')[-1].split('.')[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "n_categories = len(all_categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEvaP_5wVYPE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посмотрим на результат:"
      ]
    },
    {
      "metadata": {
        "id": "Pa1Ga0SRU0Qa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "[(x, category_lines[x][:3]) for x in category_lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dcFgEy7YeFw0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Сконвертируем датасет:"
      ]
    },
    {
      "metadata": {
        "id": "K9KvMRVFYHZ2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "words_count = sum(len(category_lines[x]) for x in category_lines)\n",
        "max_word_len = max(max(len(w) for w in category_lines[x]) for x in category_lines)\n",
        "\n",
        "X = np.zeros((words_count, max_word_len))\n",
        "y = np.zeros(words_count)\n",
        "i = 0\n",
        "for category_ind, category in enumerate(category_lines):\n",
        "    for word in category_lines[category]:\n",
        "        X[i, :len(word)] = [char2index[symb] for symb in word]\n",
        "        y[i] = category_ind\n",
        "        i += 1\n",
        "        \n",
        "indices = np.random.permutation(np.arange(X.shape[0]))\n",
        "\n",
        "X = X[indices]\n",
        "y = y[indices]\n",
        "\n",
        "X_train, y_train = X[: 3 * X.shape[0] // 4], y[: 3 * X.shape[0] // 4]\n",
        "X_val, y_val = X[3 * X.shape[0] // 4 :], y[3 * X.shape[0] // 4 :]\n",
        "\n",
        "X_train, X_val = LongTensor(X_train), LongTensor(X_val)\n",
        "y_train, y_val = LongTensor(y_train), LongTensor(y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8T_PykFePXR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А теперь задумаемся. Какая нам нужна размерность, чтобы подать тензор на вход LSTM?"
      ]
    },
    {
      "metadata": {
        "id": "2oUU6rSidDrv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_batches(dataset, batch_size):\n",
        "    X, y = dataset\n",
        "    n_samples = X.shape[0]\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_idx = indices[start:end]\n",
        "        \n",
        "        X_batch = X[batch_idx, ].transpose(1, 0)\n",
        "        y_batch = y[batch_idx, ]\n",
        "        \n",
        "        yield Variable(X_batch), Variable(y_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dnkAUetgs6Tr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Обратите внимание на `transpose`!\n",
        "\n",
        "Построим LSTM модель."
      ]
    },
    {
      "metadata": {
        "id": "Zk3OSidVS_px",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class LstmClassifier(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_embedding_dim, \n",
        "                 lstm_hidden_dim, classes_count, batch_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
        "        self.lstm = nn.LSTM(char_embedding_dim, lstm_hidden_dim)\n",
        "        self.output = nn.Linear(lstm_hidden_dim, classes_count)\n",
        "        \n",
        "        self.hidden = self.init_hidden(batch_size)\n",
        "        \n",
        "    def forward(self, word):\n",
        "        chars = self.char_embedding(word)\n",
        "        \n",
        "        _, self.hidden = self.lstm(chars, self.hidden)\n",
        "        \n",
        "        pred = self.output(self.hidden[0])\n",
        "        return pred.view(pred.size()[1:])\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        return (Variable(torch.zeros(1, batch_size, self.lstm_hidden_dim).cuda()),\n",
        "                Variable(torch.zeros(1, batch_size, self.lstm_hidden_dim).cuda()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x9vBDF2gbypR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "model = LstmClassifier(len(char2index) + 1, 32, 64, n_categories, BATCH_SIZE)\n",
        "model = model.cuda()\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "total_loss = 0\n",
        "for _ in range(100):\n",
        "    for i, (X_batch, y_batch) in enumerate(get_batches((X_train, y_train), BATCH_SIZE)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        model.hidden = model.init_hidden(y_batch.size()[0])\n",
        "\n",
        "        preds = model(X_batch)\n",
        "\n",
        "        loss = loss_function(preds, y_batch)\n",
        "        loss.backward()\n",
        "        total_loss += loss.data[0]\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if i != 0 and i % 100 == 0:\n",
        "            print(total_loss / 100)\n",
        "            total_loss = 0\n",
        "            \n",
        "            <evaluate model on val set, please>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jC76XyGjigFx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Напишите функцию для тестирования полученной сети: пусть она принимает слово и говорит, в каком языке с какой вероятностью это может быть фамилией.\n",
        "\n",
        "**Задание** Важным видом RNN является Bidirectional RNN. По сути это две RNN, одна обходит последовательность слева направо, вторая - наоборот. \n",
        "\n",
        "В результате для каждого момента времени у нас есть вектор $h_t = [f_t; b_t]$ - конкатенация (или какая-то ещё функция от $f_t$ и $b_t$) состояний $f_t$ и $b_t$ - прямого и обратного прохода последовательности. В сумме они покрывают весь контекст.\n",
        "\n",
        "В нашей задаче Bidirectional вариант может помочь тем, что сеть будет меньше забывать, с чего начиналась последовательность. То есть нам нужно будет взять $f_N$ и $b_N$ состояния: первое - последнее состояние в проходе слева направо, т.е. выход от последнего символа. Второе - последнее состояние при обратно проходе, т.е. выход для первого символа.\n",
        "\n",
        "Реализуйте Bidirectional классификатор. Для этого в `LSTM` есть параметр `bidirectional`.\n",
        "\n",
        "\n",
        "**Задание** Сейчас память расходуется очень неоптимально: мы создали батчи длиной равно длине максимального слова. Лучше разбить все слова по группам, например: слова с длиной до 4, до 8, до 16 и больше 16. \n",
        "\n",
        "При этом стоит варьировать и размер батча. Например, 32 - для 16, 64 - для 8 и т.д. То есть число символов в батче должно быть константным.\n",
        "\n",
        "Реализуйте такой вариант обучения."
      ]
    },
    {
      "metadata": {
        "id": "oDA7xqNll_Pq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Генерация имён"
      ]
    },
    {
      "metadata": {
        "id": "be97EagcmDn1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Просто классифицировать - не интересно. Давайте попробуем генерировать имена из нужного языка!\n",
        "\n",
        "Хотим получить что-нибудь такое:\n",
        "\n",
        "![generation](https://i.imgur.com/JH58tXY.png)\n",
        "\n",
        "То есть для каждого символа мы будем учиться предсказывать следующий - до тех пор, пока не сгенерируем символ конца последовательности `EOS`.\n",
        "\n",
        "Начнём, как всегда, с построения батчей."
      ]
    },
    {
      "metadata": {
        "id": "33zZLCTttv3P",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "X = np.zeros((words_count, max_word_len))\n",
        "y = np.zeros((words_count, max_word_len))\n",
        "X_category = np.zeros((words_count, n_categories))\n",
        "i = 0\n",
        "for category_ind, category in enumerate(category_lines):\n",
        "    for word in category_lines[category]:\n",
        "        X[i, :len(word)] = [char2index[symb] for symb in word]\n",
        "        y[i, :len(word)] = [char2index[symb] for symb in word[1:]] + [char2index['</s>']]\n",
        "        X_category[i, category_ind] = 1\n",
        "        i += 1\n",
        "        \n",
        "indices = np.random.permutation(np.arange(X.shape[0]))\n",
        "\n",
        "X = X[indices]\n",
        "y = y[indices]\n",
        "X_category = X_category[indices]\n",
        "\n",
        "train_size = 3 * X.shape[0] // 4\n",
        "X_train, y_train, X_category_train = X[: train_size].T, y[: train_size].T, X_category[: train_size]\n",
        "X_val, y_val, X_category_val = X[train_size :].T, y[train_size :].T, X_category[train_size :]\n",
        "\n",
        "X_train, X_val = LongTensor(X_train), LongTensor(X_val)\n",
        "y_train, y_val = LongTensor(y_train), LongTensor(y_val)\n",
        "X_category_train, X_category_val = FloatTensor(X_category_train), FloatTensor(X_category_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rj9V_DRlurlh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_batches(dataset, batch_size):\n",
        "    X, y, category = dataset\n",
        "    n_samples = X.shape[1]\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_idx = indices[start:end]\n",
        "        \n",
        "        X_batch = X[:, batch_idx]\n",
        "        y_batch = y[:, batch_idx]\n",
        "        category_batch = category[batch_idx, ].expand(X_batch.size()[:1] + category[batch_idx, ].size())\n",
        "        \n",
        "        yield Variable(X_batch), Variable(y_batch.view(-1)), Variable(category_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRTG84NarKyy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Обратите внимание: категория - one-hot-encoding вектор. Вообще говоря, можно было бы обучать эмбеддинги для категорий (*кстати, попробуйте*), но кажется логичным ожидать, что вектора для разных языков ортогональны друг другу.\n",
        "\n",
        "Напишем код для класса-генератора.\n",
        "\n",
        "Теперь нам нужен уже не выход $h_n$ из пары $(h_n, c_n)$ - финальное состояние, которым мы пользовались раньше, а весь $\\text{output} = (h_1, \\ldots, h_n)$ - состояния `lstm` для всех моментов времени. Это первый элемент, возвращаемый `lstm`. \n",
        "\n",
        "По ним мы и будем предсказывать символ: символ в позиции $t$ должен получаться сэмлированием из вектора, заданного линейным преобразованием состояния `lstm` $h_t$: $\\ \\text{symb}_t \\sim W_o h_t + b_o$. (Это линейное преобразование задает `self.output`).\n",
        "\n",
        "Подумайте, какие размерности будет иметь выход из `lstm` и результат его обработки с помощью полносвязного слоя."
      ]
    },
    {
      "metadata": {
        "id": "8FY23DOawVg6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class LstmGenerator(nn.Module):\n",
        "    def __init__(self, char_vocab_size, char_embedding_dim, category_count,\n",
        "                 lstm_hidden_dim, classes_count, batch_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
        "        self.lstm = nn.LSTM(char_embedding_dim + category_count, lstm_hidden_dim)\n",
        "        self.output = nn.Linear(lstm_hidden_dim, classes_count)\n",
        "        \n",
        "        self.hidden = self.init_hidden(batch_size)\n",
        "        \n",
        "    def forward(self, word, category):\n",
        "        <your code here>\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        return (Variable(torch.zeros(1, batch_size, self.lstm_hidden_dim).cuda()),\n",
        "                Variable(torch.zeros(1, batch_size, self.lstm_hidden_dim).cuda()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZUf567n8rarU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Потренируем модель. Делается это аналогично предыдущей модели."
      ]
    },
    {
      "metadata": {
        "id": "6AI__TDcw_6e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "model = LstmGenerator(len(char2index) + 1, 32, n_categories, 64, len(char2index) + 1, BATCH_SIZE)\n",
        "model = model.cuda()\n",
        "\n",
        "<your code here>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pT0KUtnFriNS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А теперь попробуем погенерировать какой-нибудь текст. Пригодится такая функция:"
      ]
    },
    {
      "metadata": {
        "id": "f3jpnzwIj0df",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = preds[0, 0] / temperature\n",
        "    preds = F.softmax(preds, dim=0).data.cpu().numpy()\n",
        "    return np.random.choice(np.arange(preds.shape[0]), p=preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFh_FfMZrq4m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Написать функцию-генератор фамилии, принимающую первую букву и язык, из которого должна быть эта фамилия.\n",
        "\n",
        "Она должна в цикле генерировать слово символ за символом, пока не выпадет `</s>`. Для генерации символа нужна функция `sample`. Обратите внимание на параметр `temperature`. Чем он ниже, тем консервативнее модель (т.е. ниже вероятность предсказать маловероятный символ). Чем выше - тем разнообразнее её предсказания.\n",
        "\n",
        "Обратите внимание: нам не нужно подавать в сеть весь предыдущий предсказанный контекст. Например, если модель только что предсказала $p_{t-1}$ - распределение вероятностей для $t-1$ позиции, она также помнит и $(h_{t-1}, c_{t-1})$ - вектора, содержащие состояние модели. Они записаны в `self.hidden`.\n",
        "\n",
        "Тогда достаточно передать модели $c_{t-1}$ - сэмплированный из $p_{t-1}$ элемент, чтобы она смогла предсказать $p_{t}$. То есть передавать в модель будем на каждом шаге тензор с одним элементом, а обнулять `self.hidden` - только перед началом генерации."
      ]
    },
    {
      "metadata": {
        "id": "Z2anv2mWhmLG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_surname(model, category=1, first_letter='A', temperature=0.5):\n",
        "    <your code here>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nmpef439-8xA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Безусловная генерация текстов"
      ]
    },
    {
      "metadata": {
        "id": "-hikEnkN_EbV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Вообще говоря, то, что вы видели выше - условная генерация текста. Мы там генерировали фамилии при условии заданного языка. Есть и более простая разновидность генерации текстов - безусловная.\n",
        "\n",
        "Попробуем погенерировать Шекспира!"
      ]
    },
    {
      "metadata": {
        "id": "XMHfkS3__cBq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget --quiet -c -O shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zvGCqLsx_qoy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Файл выглядит так:"
      ]
    },
    {
      "metadata": {
        "id": "UjsQ8YZX_n5q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ec9e4447-ab41-422d-d223-4fe5c720b599",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520552550183,
          "user_tz": -180,
          "elapsed": 889,
          "user": {
            "displayName": "Daniil Anastasyev",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "101608282221277341857"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!head shakespeare.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\r\n",
            "Before we proceed any further, hear me speak.\r\n",
            "\r\n",
            "All:\r\n",
            "Speak, speak.\r\n",
            "\r\n",
            "First Citizen:\r\n",
            "You are all resolved rather to die than to famish?\r\n",
            "\r\n",
            "All:\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UOMG5Z6O_36u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Постройте модель, которая будет генерировать текст, обучившись на Шекспире.\n",
        "\n",
        "Это должна быть символьная модель - совсем как те две, которые мы делали перед этим.\n",
        "\n",
        "В качестве батча нужно будет подавать тензоры размерности `(sequence_length, batch_size)`.\n",
        "\n",
        "Скорее всего, правильнее будет превратить последовательность символов, записанную в `shakespeare.txt` в вектор индексов этих символов длины `N`. \n",
        "\n",
        "Дальше - превратить его в матрицу `(N // BS, BS)`, где `BS` - выбранный размер батча. Для этого подойдет простой вызов `view`. Только нужно перед ним либо обрезать `N` до значения, делящегося нацело на `BS`, либо наоборот - дополнить нулями вектор индексов.\n",
        "\n",
        "Например, если изначальная последовательность была просто последовательностью всех букв от `a` до `z`, матрица может быть следующей при размере батча 4:\n",
        "```\n",
        "┌ a g m s ┐\n",
        "│ b h n t │\n",
        "│ c i o u │\n",
        "│ d j p v │\n",
        "│ e k q w │\n",
        "└ f l r x ┘\n",
        "```\n",
        "\n",
        "Затем нам нужно будет формировать мини-батч просто двигаясь по этой матрице. \n",
        "\n",
        "При длине последовательности 2 наши `X` и `y` будут такими:\n",
        "```\n",
        "    ┌ a g m s ┐       ┌ b h n t ┐\n",
        "X = └ b h n t ┘,  y = └ c i o u ┘\n",
        "```\n",
        "\n",
        "При этом hidden state мы можем сохранять - в начале нового мини-батча моделе будет полезно знать, чем закончился предыдущий. Для этого пригодится такая функция:"
      ]
    },
    {
      "metadata": {
        "id": "XsbKYOD6CFL5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
        "    if type(h) == Variable:\n",
        "        return Variable(h.data)\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S-hy-ZWPDm2Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Have fun! :)"
      ]
    }
  ]
}