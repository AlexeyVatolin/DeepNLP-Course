{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 6 - Convs.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dF1fio53UKN6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.cuda import FloatTensor, LongTensor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q3lN5pl5Stpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Свёрточные нейронные сети"
      ]
    },
    {
      "metadata": {
        "id": "rRhcCJcAS4S5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Напомню, свертки - это то, с чего начался хайп нейронных сетей в районе 2012-ого.\n",
        "\n",
        "Работают они примерно так:  \n",
        "![Conv example](http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif)   \n",
        "From [Feature extraction using convolution](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution).\n",
        "\n",
        "Формально - учатся наборы фильтров, каждый из которых скалярно умножается на элементы матрицы признаков. На картинке выше исходная матрица сворачивается с фильтром\n",
        "$$\n",
        " \\begin{pmatrix}\n",
        "  1 & 0 & 1 \\\\\n",
        "  0 & 1 & 0 \\\\\n",
        "  1 & 0 & 1\n",
        " \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Но нужно не забывать, что свертки обычно имеют ещё такую размерность, как число каналов. Например, картинки имеют обычно три канала: RGB.  \n",
        "Наглядно демонстрируется как выглядят при этом фильтры [здесь](http://cs231n.github.io/convolutional-networks/#conv).\n",
        "\n",
        "После сверток обычно следуют pooling-слои. Они помогают уменьшить размерность тензора, с которым приходится работать. Самым частым является max-pooling:  \n",
        "![maxpooling](http://cs231n.github.io/assets/cnn/maxpool.jpeg =x300)  \n",
        "From [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/#pool)"
      ]
    },
    {
      "metadata": {
        "id": "x-M3lCE1ealB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Character-Level Convolutions\n",
        "Мы говорим про свертки для работы с текстами. Совсем не очевидно, что они вообще должны помочь в работе с текстами. То есть в изображениях они отлавливают некоторые локальные особенности, такие как:\n",
        "![weights](https://i.stack.imgur.com/Hl2H6.png)\n",
        "\n",
        "Для текстов свертки работают как n-граммные детекторы (примерно). Посмотрите на пример символьной сверточной сети:\n",
        "\n",
        "![text-convs](https://image.ibb.co/bC3Xun/2018_03_27_01_24_39.png =x500)  \n",
        "From [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)\n",
        "\n",
        "*Сколько учится фильтров на данном примере?*\n",
        "\n",
        "На картинке показано, как из слова извлекаются 2, 3 и 4-граммы. Например, желтые - это триграммы. Желтый фильтр прикладывают ко всем триграммам в слове, а потом с помощью global max-pooling извлекают наиболее сильный сигнал.\n",
        "\n",
        "Что это значит, если конкретнее?\n",
        "\n",
        "Каждый символ отображается с помощью эмбеддингов в некоторый вектор. А их последовательности - в конкатенации эмбеддингов.  \n",
        "Например, \"abs\" $\\to [v_a; v_b; v_s] \\in \\mathbb{R}^{3 d}$, где $d$ - размерность эмбеддинга. Желтый фильтр $f_k$ имеет такую же размерность $3d$.  \n",
        "Его прикладывание - это скалярное произведение $\\left([v_a; v_b; v_s] \\odot f_k \\right) \\in \\mathbb R$ (один из желтых квадратиков в feature map для данного фильтра).\n",
        "\n",
        "Max-pooling выбирает $max_i \\left( [v_{i-1}; v_{i}; v_{i+1}] \\odot f_k \\right)$, где $i$ пробегается по всем индексам слова от 1 до $|w| - 1$ (либо по большему диапазону, если есть padding'и).   \n",
        "Этот максимум соответствует той триграмме, которая наиболее близка к фильтру по косинусному расстоянию.\n",
        "\n",
        "В результате в векторе после max-pooling'а закодирована информация о том, какие из n-грамм встретились в слове: если встретилась близкая к нашему $f_k$ триграмма, то в $k$-той позиции вектора будет стоять большое значение, иначе - маленькое.\n",
        "\n",
        "А учим мы как раз фильтры. То есть сеть должна научиться определять, какие из n-грамм значимы, а какие - нет."
      ]
    },
    {
      "metadata": {
        "id": "9J6KBv-cniLw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Классификация слов\n",
        "\n",
        "Будем учиться предсказывать, является ли слово фамилией.\n",
        "\n",
        "Скачаем данные."
      ]
    },
    {
      "metadata": {
        "id": "x6NEERMkR__8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -qq -O surnames.txt https://share.abbyy.com/index.php/s/mt5r9vEZo70sfIS/download"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qFUvFUgHTs47",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with open('surnames.txt') as f:\n",
        "    lines = f.readlines()\n",
        "    data = [line.strip().split('\\t')[0] for line in lines]\n",
        "    labels = [int(line.strip().split('\\t')[1]) for line in lines]\n",
        "    del lines\n",
        "    \n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfUIgqMznrXB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Данные, как видно, грязноваты:"
      ]
    },
    {
      "metadata": {
        "id": "YN1XYen8UauD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "list(zip(X_train, y_train))[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LJJtiGMK1ATy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Начнем с бейзлайна - логистической регрессии на n-граммах символов."
      ]
    },
    {
      "metadata": {
        "id": "8COAoh7b0TXs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3,3), lowercase=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h7ZVbNOGoV5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Как всегда, сконвертируем их для начала:"
      ]
    },
    {
      "metadata": {
        "id": "DEtZ6g78Wtj-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter \n",
        "    \n",
        "def find_max_len(counter, threshold):\n",
        "    sum_count = sum(counter.values())\n",
        "    cum_count = 0\n",
        "    for i in range(max(counter)):\n",
        "        cum_count += counter[i]\n",
        "        if cum_count > sum_count * threshold:\n",
        "            return i\n",
        "    return max(counter)\n",
        "\n",
        "word_len_counter = Counter()\n",
        "for word in X_train:\n",
        "    word_len_counter[len(word)] += 1\n",
        "    \n",
        "threshold = 0.99\n",
        "MAX_WORD_LEN = find_max_len(word_len_counter, threshold)\n",
        "\n",
        "print('Max word len for {:.0%} of words is {}'.format(threshold, MAX_WORD_LEN))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YyMoPEXGVs3s",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "chars = set()\n",
        "for word in X_train:\n",
        "    chars.update(word)\n",
        "\n",
        "char_index = {c : i + 1 for i, c in enumerate(chars)}\n",
        "char_index[''] = 0\n",
        "\n",
        "def get_char_index(char, char_index):\n",
        "    return char_index[char] if char in char_index else len(char_index)\n",
        "  \n",
        "print(char_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qETmYKm8W_TX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def convert_data(data, max_word_len, char_index):\n",
        "    X = np.zeros((len(data), max_word_len))\n",
        "    for i, word in enumerate(data):\n",
        "        word = word[-max_word_len:]\n",
        "        X[i, :len(word)] = [get_char_index(symb, char_index) for symb in word]\n",
        "        \n",
        "    return LongTensor(X)\n",
        "  \n",
        "X_train = convert_data(X_train, MAX_WORD_LEN, char_index)\n",
        "X_test = convert_data(X_test, MAX_WORD_LEN, char_index)\n",
        "\n",
        "y_train = FloatTensor(y_train)\n",
        "y_test = FloatTensor(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VufrP006Vk-y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def iterate_batches(dataset, batch_size):\n",
        "    X, y = dataset\n",
        "    num_samples = X.shape[0]\n",
        "\n",
        "    indices = np.arange(num_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, num_samples, batch_size):\n",
        "        end = min(start + batch_size, num_samples)\n",
        "        \n",
        "        batch_idx = indices[start:end]\n",
        "        \n",
        "        yield Variable(X[batch_idx, ]), Variable(y[batch_idx, ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gkXlKB7lobYE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь построим свёрточную модель.\n",
        "\n",
        "Типичным является блок:\n",
        "```python\n",
        "nn.Conv*d(in_channels=N, out_channels=M, kernel_size=K1, padding=0)\n",
        "F.relu\n",
        "nn.MaxPool*d(kernel_size=K2)\n",
        "```\n",
        "\n",
        "Пусть она будет строить триграммы - то есть применять фильтры на 3 символа.\n",
        "\n",
        "Какие нам нужны размерности?"
      ]
    },
    {
      "metadata": {
        "id": "v2T4AorsZ530",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ConvClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, filters_count):\n",
        "        super().__init__()\n",
        "        \n",
        "        <init layers>\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        '''\n",
        "        inp.size() = (batch_size, max_word_len)\n",
        "        out.size() = (batch_size,)\n",
        "        '''\n",
        "        <apply layers>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JqiBVpBwqmb_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В данной задаче несбалансированные классы, поэтому хочется мерять $F_1$-меру.\n",
        "\n",
        "Напомню:\n",
        "\n",
        "![precision-recall](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png =x600)  \n",
        "From [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
        "\n",
        "$$\\text{precision} = \\frac{tp}{tp + fp}.$$\n",
        "$$\\text{recall} = \\frac{tp}{tp + fn}.$$\n",
        "$$\\text{F}_1 = 2\\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}.$$"
      ]
    },
    {
      "metadata": {
        "id": "NJOnH2-rqmJu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = ConvClassifier(len(char_index) + 1, 24, 128).cuda()\n",
        "\n",
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 32))\n",
        "\n",
        "<calculate precision, recall and F1-score>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MsCtTJucVjMH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None):\n",
        "    epoch_loss = 0\n",
        "    epoch_tp = 0\n",
        "    epoch_tpfn = 0\n",
        "    epoch_tpfp = 0\n",
        "    \n",
        "    model.train(not optimizer is None)\n",
        "    \n",
        "    batchs_count = math.ceil(data[0].shape[0] / batch_size)\n",
        "    \n",
        "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "        logits = model(X_batch)\n",
        "        \n",
        "        loss = criterion(logits, y_batch)\n",
        "        epoch_loss += loss.data[0]\n",
        "        \n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        <calculate precision, recall and F1-score for batch>\n",
        "      \n",
        "        print('\\r[{} / {}]: Loss = {:.5f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'.format(\n",
        "              i, batchs_count, loss.data[0], precision, recall, f1), end='')\n",
        "        \n",
        "    <calculate precision, recall and F1-score for epoch>\n",
        "        \n",
        "    return epoch_loss / batchs_count, recall, precision, f1\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
        "        batch_size=32, val_data=None, val_batch_size=None):\n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_recall, train_precision, train_f1 = \\\n",
        "            do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
        "        \n",
        "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.5f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
        "        if not val_data is None:\n",
        "            val_loss, val_recall, val_precision, val_f1 = \\\n",
        "                do_epoch(model, criterion, train_data, batch_size, None)\n",
        "            \n",
        "            epoch_time = time.time() - start_time\n",
        "            output_info += ', Val Loss = {:.5f}, Precision = {:.2%}, Recall = {:.2%}, F1 = {:.2%}'\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, \n",
        "                                     train_loss, train_recall, train_precision, train_f1,\n",
        "                                     val_loss, val_recall, val_precision, val_f1))\n",
        "        else:\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlq63hAXh0Gr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = ConvClassifier(len(char_index) + 1, 24, 256).cuda()\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().cuda()\n",
        "\n",
        "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad])\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=100, \n",
        "    batch_size=512, val_data=(X_test, y_test), val_batch_size=1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qstoDysVsSQ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Различают Narrow и Wide свёртки - по сути, добавляется ли нулевой паддинг или нет. Для текстов эта разница выглядит так:  \n",
        "![narrow_vs_wide](https://image.ibb.co/eqGZaS/2018_03_28_11_23_17.png)\n",
        "From Neural Network Methods in Natural Language Processing.  \n",
        "Слева - паддинг отсутствует, справа - есть. Попробуйте добавить паддинг и посмотреть, что получится. Потенциально он поможет выучить хорошие префиксы слова.\n",
        "\n",
        "--- \n",
        "\n",
        "**Задание** Сравните качество и скорость работы с character-level LSTM (типа того, что был на третьем занятии)."
      ]
    },
    {
      "metadata": {
        "id": "xOt1tcOkvDBY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Визуализация полученных свёрток"
      ]
    },
    {
      "metadata": {
        "id": "6NO_aVBDvKcT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Мы обучили набор свёрток и эмбеддинги. Давайте посмотрим, на какие именно символы загораются свёртки."
      ]
    },
    {
      "metadata": {
        "id": "DA0ndcTJ0O-s",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "filters = next(model.conv.parameters())\n",
        "embeddings = next(model.embedding.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqutnmzWvdUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Рассмотрим только маленькие буквы:"
      ]
    },
    {
      "metadata": {
        "id": "sSAIKGCcF156",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_range(first_symb, last_symb):\n",
        "    return [chr(c) for c in range(ord(first_symb), ord(last_symb) + 1)]\n",
        "  \n",
        "russian_letters = [''] + get_range('а', 'я')\n",
        "russian_letters_idx = [char_index[letter] for letter in russian_letters]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q0b7Fwp2vpT7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Эмбеддинг триграммы - это просто конкатенация эмбеддингов её символов:"
      ]
    },
    {
      "metadata": {
        "id": "8b8h1MOFAzhw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "suffix = 'сев'\n",
        "\n",
        "suffix_embedding = torch.cat([embeddings[char_index[letter]] for letter in suffix] + \n",
        "                             [embeddings[char_index['']] for i in range(3 - len(suffix))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbJ8IVlrv5oh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посчитайте, какой из фильтров сильнее всего реагирует на триграмму."
      ]
    },
    {
      "metadata": {
        "id": "M0wdpX3EwJxA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "<Find the most similar filter>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bmtp3FDEwOwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А теперь - наоборот, подсчитаем, как найденный фильтр реагирует на все символы из `russian_letters`.\n",
        "\n",
        "Нужно построить матрицу `sim` размера `3 x len(russian_letters)`, в каждом элементе которой будет записано, насколько сильно данный элемент фильтра реагирует на данный символ."
      ]
    },
    {
      "metadata": {
        "id": "8oT2ml0F2gVt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "sim = ...\n",
        "\n",
        "fig = plt.figure(figsize=(30, 5))\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(sim, cmap='bone')\n",
        "fig.colorbar(cax)\n",
        "\n",
        "ax.set_xticklabels([''] + russian_letters)\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JXWmMkWMfUYZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Attention and Convolutions"
      ]
    },
    {
      "metadata": {
        "id": "1WXv4ObuZUCN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Напомню, что attention работает так: пусть у нас есть набор скрытых состояний $\\mathbf{s}_1, \\ldots, \\mathbf{s}_m$ - например, представлений слов из исходного языка, полученных с помощью энкодера. И есть некоторое текущее скрытое состояние $\\mathbf{h}_i$ - скажем, представление, используемое для предсказания слова на нужном нам языке.\n",
        "\n",
        "Тогда с помощью аттеншена мы можем получить взвешенное представление контекста $\\mathbf{s}_1, \\ldots, \\mathbf{s}_m$ - вектор $c_i$:\n",
        "$$\n",
        "\\begin{align}\\begin{split}\n",
        "\\mathbf{c}_i &= \\sum\\limits_j a_{ij}\\mathbf{s}_j\\\\\n",
        "\\mathbf{a}_{ij} &= \\text{softmax}(f_{att}(\\mathbf{h}_i, \\mathbf{s}_j))\n",
        "\\end{split}\\end{align}\n",
        "$$\n",
        "\n",
        "$f_{att}$ - функция, которая говорит, насколько хорошо $\\mathbf{h}_i$ и $\\mathbf{s}_j$ подходят друг другу.\n",
        "\n",
        "Самые популярные её варианты:\n",
        "- Additive attention:\n",
        "$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{v}_a{}^\\top \\text{tanh}(\\mathbf{W}_a[\\mathbf{h}_i; \\mathbf{s}_j])$$\n",
        "- Dot attention:\n",
        "$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{h}_i^\\top \\mathbf{s}_j$$\n",
        "- Multiplicative attention:\n",
        "$$f_{att}(\\mathbf{h}_i, \\mathbf{s}_j) = \\mathbf{h}_i^\\top \\mathbf{W}_a \\mathbf{s}_j$$\n",
        "\n",
        "Есть ещё одна вариация на тему - self-attention. Это когда у нас нет $\\mathbf{h}_i$ - только $\\mathbf{s}_j$-тые, т.е. вектора-представления исходной последовательности. Такое может очень естественно возникнуть практически в любой задаче - например, в классификации текстов.\n",
        "\n",
        "Additive self-attention можно записать как $f_{att}(\\mathbf{s}_i) = \\mathbf{v}_a{}^\\top \\text{tanh}(\\mathbf{W}_a \\mathbf{s}_i)$. Тогда по последовательности $\\mathbf{s}_1, \\ldots, \\mathbf{s}_m = \\mathbf{S}$ вычисляется единственный вектор:\n",
        "$$\n",
        "\\begin{align}\\begin{split}\n",
        "\\mathbf{a} &= \\text{softmax}(\\mathbf{v}_a \\text{tanh}(\\mathbf{W}_a \\mathbf{S}^\\top))\\\\\n",
        "\\mathbf{c} & = \\mathbf{S} \\mathbf{a}^\\top\n",
        "\\end{split}\\end{align}\n",
        "$$\n",
        "\n",
        "При чём здесь были упомянуты свёртки? В основном при том, что они, вообще говоря, действуют похожим образом. \n",
        "\n",
        "Давайте посмотрим на операцию $\\mathbf{v}_a \\text{tanh}(\\mathbf{W}_a \\mathbf{S}^\\top)$ как на извращенный вариант свертки с размером фильтра 1. Для каждого вектора $\\mathbf{s_j}$ строится некоторая оценка того, насколько важно данное состояние.\n",
        "\n",
        "Кроме того, кроме max-pooling'а существует ещё и avg-pooling - когда берется не максимальный сигнал, полученный сверткой, а средний. В какой-то степени $\\mathbf{c} = \\mathbf{S} \\mathbf{a}^\\top$ - тоже усреднение. Правда, усредняются исходные вектора, а не получающиеся оценки.\n",
        "\n",
        "В таком self-attention мы выучиваем только один аналог фильтра сверточной сети. Но в сверточных сетях же много фильтров. Давайте тогда учить сразу несколько вариантов attention'а:\n",
        "$$\n",
        "\\begin{align}\\begin{split}\n",
        "\\mathbf{A} &= \\text{softmax}(\\mathbf{V}_a \\text{tanh}(\\mathbf{W}_a \\mathbf{H}^\\top))\\\\\n",
        "\\mathbf{C} & = \\mathbf{A} \\mathbf{H}\n",
        "\\end{split}\\end{align}\n",
        "$$\n",
        "\n",
        "Утверждается, что случайной инициализации вполне достаточно, чтобы attention начал отлавливать разные характеристики из последовательности - такие лексическое и грамматическое значение слов. Но это и вполне естественно, если посмотреть, что свертки уже много лет так учат."
      ]
    },
    {
      "metadata": {
        "id": "jQ8WR867xprA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Attention Is All You Need\n",
        "\n",
        "В середине прошлого года вышла статья [Attention Is All You Need](https://arxiv.org/abs/1706.03762), которая опирается на эти идеи.\n",
        "\n",
        "Она описывает Transformer - архитектуру полносвязной сети, которая делает то, что раньше все делали с помощью рекуррентных сетей. Хороший обзор был на хабре: [Transformer — новая архитектура нейросетей для работы с последовательностями](https://habrahabr.ru/post/341240/).\n",
        "\n",
        "К задаче машинного перевода она применима так:  \n",
        "![transformer](https://hsto.org/webt/59/f0/44/59f04410c0e56192990801.png =x600)\n",
        "\n",
        "Из интересного - multi-head attention, который является аналогом того, что мы рассматривали в предыдущем разделе:  \n",
        "![multi-head attn](https://hsto.org/webt/59/f0/44/59f0440f1109b864893781.png)\n",
        "\n",
        "Сам attention выглядит так: \n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V,$$\n",
        "\n",
        "Выглядит страшно, но в реальности передают в качестве $Q, K, V$ одну и ту же последовательность скрытых состояний - $\\mathbf{S}$ из предыдущего раздела.\n",
        "\n",
        "$Q K^\\top$ - это как dot-attention.\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^o$$ \n",
        "$$\\text{head}_i = \\text{Attention}(Q W^Q_i, K W^K_i, V W^V_i).$$\n",
        "\n",
        "На pytorch это выглядит так (утащено из [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch) и [seq2seq.pytorch](https://github.com/eladhoffer/seq2seq.pytorch)):"
      ]
    },
    {
      "metadata": {
        "id": "ikj6l4vofT7Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dropout=0, causal=False):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        b_q, t_q, dim_q = q.size()\n",
        "        b_k, t_k, dim_k = k.size()\n",
        "        b_v, t_v, dim_v = v.size()\n",
        "        qk = torch.bmm(q, k.transpose(1, 2))  # b x t_q x t_k\n",
        "        qk.div_(dim_k ** 0.5)\n",
        "        sm_qk = F.softmax(qk, dim=2)\n",
        "        sm_qk = self.dropout(sm_qk)\n",
        "        return torch.bmm(sm_qk, v), sm_qk  # b x t_q x dim_v\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, num_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert(input_size % num_heads == 0)\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        self.linear_q = nn.Linear(input_size, input_size)\n",
        "        self.linear_k = nn.Linear(input_size, input_size)\n",
        "        self.linear_v = nn.Linear(input_size, input_size)\n",
        "        self.linear_out = nn.Linear(input_size, output_size)\n",
        "        \n",
        "        self.sdp_attention = ScaledDotProductAttention(dropout=dropout)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        b_q, t_q, dim_q = q.size()\n",
        "        b_k, t_k, dim_k = k.size()\n",
        "        b_v, t_v, dim_v = v.size()\n",
        "        \n",
        "        qw = self.linear_q(q)\n",
        "        kw = self.linear_k(k)\n",
        "        vw = self.linear_v(v)\n",
        "        \n",
        "        qw = qw.chunk(self.num_heads, 2)\n",
        "        kw = kw.chunk(self.num_heads, 2)\n",
        "        vw = vw.chunk(self.num_heads, 2)\n",
        "        \n",
        "        output = []\n",
        "        attention_scores = []\n",
        "        for i in range(self.num_heads):\n",
        "            out_h, score = self.sdp_attention(qw[i], kw[i], vw[i])\n",
        "            output.append(out_h)\n",
        "            attention_scores.append(score)\n",
        "\n",
        "        output = torch.cat(output, 2)\n",
        "\n",
        "        return self.linear_out(output), attention_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0la3q01F3Au2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Чтобы всё это заработало, нужно смотреть не только на эмбеддинги слов, но и на их позиции в тексте. Предлагается использовать позиционное кодирование вида:\n",
        "$$\\text{PE}_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})$$\n",
        "$$\\text{PE}_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})$$\n",
        "\n",
        "Хотя можно и просто учить эмбеддинги.\n",
        "\n",
        "Их можно предподсчитать:"
      ]
    },
    {
      "metadata": {
        "id": "7epSQsRb3i_3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def position_encoding_init(n_position, d_pos_vec):\n",
        "    ''' Init the sinusoid position encoding table '''\n",
        "\n",
        "    # keep dim 0 for padding token position encoding zero vector\n",
        "    position_enc = np.array([\n",
        "        [pos / np.power(10000, 2 * (j // 2) / d_pos_vec) for j in range(d_pos_vec)]\n",
        "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
        "\n",
        "    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n",
        "    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n",
        "    return torch.from_numpy(position_enc).type(FloatTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eTeCJDXxTdp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Энкодер выглядит просто:"
      ]
    },
    {
      "metadata": {
        "id": "uUcF9_nKxZzp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=512, num_heads=8, inner_linear=1024, layer_norm=True, dropout=0):\n",
        "\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        if layer_norm:\n",
        "            self.lnorm1 = LayerNorm1d(hidden_size)\n",
        "            self.lnorm2 = LayerNorm1d(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.attention = MultiHeadAttention(hidden_size, hidden_size, num_heads, dropout=dropout)\n",
        "        self.fc = nn.Sequential(nn.Linear(hidden_size, inner_linear),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Dropout(dropout),\n",
        "                                nn.Linear(inner_linear, hidden_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        x, _ = self.attention(x, x, x)\n",
        "        x = self.dropout(x).add_(res)\n",
        "        x = self.lnorm1(x) if hasattr(self, 'lnorm1') else x\n",
        "        res = x\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x).add_(res)\n",
        "        x = self.lnorm2(x) if hasattr(self, 'lnorm2') else x\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHKbLtMvyFH8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Декодер отличается только необходимостью добавить маскинг - он должен смотреть только на предыдущие сгенерированные состояния, но не на следующие, и attention'ом на выход энкодера."
      ]
    }
  ]
}